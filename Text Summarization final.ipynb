{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"food review/Reviews.csv\")[:35173]\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35173 entries, 0 to 35172\n",
      "Data columns (total 10 columns):\n",
      "Id                        35173 non-null int64\n",
      "ProductId                 35173 non-null object\n",
      "UserId                    35173 non-null object\n",
      "ProfileName               35172 non-null object\n",
      "HelpfulnessNumerator      35173 non-null int64\n",
      "HelpfulnessDenominator    35173 non-null int64\n",
      "Score                     35173 non-null int64\n",
      "Time                      35173 non-null int64\n",
      "Summary                   35172 non-null object\n",
      "Text                      35173 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               1\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   1\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)\n",
    "reviews = reviews.dropna()\n",
    "demo_summary= reviews.drop(['Text'],1)\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    0\n",
       "Text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35172, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count          35172\n",
      "unique         28194\n",
      "top       Delicious!\n",
      "freq             136\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "count                                                 35172\n",
      "unique                                                33043\n",
      "top       Diamond Almonds<br />Almonds are a good source...\n",
      "freq                                                     12\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(reviews.Summary.describe())\n",
    "print()\n",
    "print(reviews.Text.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(str(summary), remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(str(text), remove_stopwords=True))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summaries:  good quality dog food\n",
      "\n",
      "Text:  product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summaries:  not as advertised\n",
      "\n",
      "Text:  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summaries:   delight  says it all\n",
      "\n",
      "Text:  looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "Summaries:  cough medicine\n",
      "\n",
      "Text:  great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summaries:  great taffy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Text: \", clean_texts[i])\n",
    "    print(\"Summaries: \",clean_summaries[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 37145\n"
     ]
    }
   ],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1\n",
    "                \n",
    "'''Find the number of times each word was used and the size of the vocabulary'''\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open('numberbatch-en/numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 251\n",
      "Percent of words that are missing from vocabulary: 0.6799999999999999%\n"
     ]
    }
   ],
   "source": [
    "'''Find the number of words that are missing from CN, and are used more than our threshold.'''\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 37145\n",
      "Number of words we will use: 26955\n",
      "Percent of words we will use: 72.57000000000001%\n"
     ]
    }
   ],
   "source": [
    "'''Limit the vocab that we will use to words that appear â‰¥ threshold or are in GloVe'''\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26955\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        #embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1563100\n",
      "Total number of UNKs in headlines: 19624\n",
      "Percent of words that are UNK: 1.26%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean       4.177101\n",
      "std        2.648076\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        4.000000\n",
      "75%        5.000000\n",
      "max       30.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean      41.264500\n",
      "std       39.651104\n",
      "min        1.000000\n",
      "25%       18.000000\n",
      "50%       29.000000\n",
      "75%       50.000000\n",
      "max      783.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31084\n",
      "31084\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 199 #84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 100 # use 1\n",
    "unk_summary_limit = 100 # use 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_units=12\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    #Attention Mechanism\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32)) \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 50 # use 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This module will train the above graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 5 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "#uncomment the above line for new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will Strat now.\n",
      "Epoch   1/50 Batch    5/485 - Loss:  6.545, Seconds: 0.49\n",
      "Epoch   1/50 Batch   10/485 - Loss:  3.219, Seconds: 0.59\n",
      "Epoch   1/50 Batch   15/485 - Loss:  2.987, Seconds: 0.50\n",
      "Epoch   1/50 Batch   20/485 - Loss:  2.648, Seconds: 0.59\n",
      "Epoch   1/50 Batch   25/485 - Loss:  2.867, Seconds: 0.48\n",
      "Epoch   1/50 Batch   30/485 - Loss:  2.660, Seconds: 0.60\n",
      "Epoch   1/50 Batch   35/485 - Loss:  2.695, Seconds: 0.55\n",
      "Epoch   1/50 Batch   40/485 - Loss:  2.792, Seconds: 0.45\n",
      "Epoch   1/50 Batch   45/485 - Loss:  2.713, Seconds: 0.58\n",
      "Epoch   1/50 Batch   50/485 - Loss:  2.651, Seconds: 0.61\n",
      "Epoch   1/50 Batch   55/485 - Loss:  2.683, Seconds: 0.43\n",
      "Epoch   1/50 Batch   60/485 - Loss:  2.719, Seconds: 0.48\n",
      "Epoch   1/50 Batch   65/485 - Loss:  2.493, Seconds: 0.51\n",
      "Epoch   1/50 Batch   70/485 - Loss:  2.402, Seconds: 0.54\n",
      "Epoch   1/50 Batch   75/485 - Loss:  2.748, Seconds: 0.59\n",
      "Epoch   1/50 Batch   80/485 - Loss:  2.737, Seconds: 0.60\n",
      "Epoch   1/50 Batch   85/485 - Loss:  2.612, Seconds: 0.54\n",
      "Epoch   1/50 Batch   90/485 - Loss:  2.624, Seconds: 0.55\n",
      "Epoch   1/50 Batch   95/485 - Loss:  2.606, Seconds: 0.54\n",
      "Epoch   1/50 Batch  100/485 - Loss:  2.341, Seconds: 0.69\n",
      "Epoch   1/50 Batch  105/485 - Loss:  2.517, Seconds: 0.68\n",
      "Epoch   1/50 Batch  110/485 - Loss:  2.598, Seconds: 0.63\n",
      "Epoch   1/50 Batch  115/485 - Loss:  2.664, Seconds: 0.64\n",
      "Epoch   1/50 Batch  120/485 - Loss:  2.544, Seconds: 0.63\n",
      "Epoch   1/50 Batch  125/485 - Loss:  2.672, Seconds: 0.59\n",
      "Epoch   1/50 Batch  130/485 - Loss:  2.501, Seconds: 0.54\n",
      "Epoch   1/50 Batch  135/485 - Loss:  2.481, Seconds: 0.68\n",
      "Epoch   1/50 Batch  140/485 - Loss:  2.714, Seconds: 0.62\n",
      "Epoch   1/50 Batch  145/485 - Loss:  2.459, Seconds: 0.64\n",
      "Epoch   1/50 Batch  150/485 - Loss:  2.656, Seconds: 0.60\n",
      "Epoch   1/50 Batch  155/485 - Loss:  2.559, Seconds: 0.65\n",
      "Epoch   1/50 Batch  160/485 - Loss:  2.352, Seconds: 0.57\n",
      "Average loss for this update: 2.764\n",
      "New Record!\n",
      "Epoch   1/50 Batch  165/485 - Loss:  2.609, Seconds: 0.59\n",
      "Epoch   1/50 Batch  170/485 - Loss:  2.601, Seconds: 0.66\n",
      "Epoch   1/50 Batch  175/485 - Loss:  2.304, Seconds: 0.68\n",
      "Epoch   1/50 Batch  180/485 - Loss:  2.491, Seconds: 0.68\n",
      "Epoch   1/50 Batch  185/485 - Loss:  2.626, Seconds: 0.61\n",
      "Epoch   1/50 Batch  190/485 - Loss:  2.608, Seconds: 0.74\n",
      "Epoch   1/50 Batch  195/485 - Loss:  2.584, Seconds: 0.62\n",
      "Epoch   1/50 Batch  200/485 - Loss:  2.781, Seconds: 0.61\n",
      "Epoch   1/50 Batch  205/485 - Loss:  2.638, Seconds: 0.59\n",
      "Epoch   1/50 Batch  210/485 - Loss:  2.688, Seconds: 0.74\n",
      "Epoch   1/50 Batch  215/485 - Loss:  2.570, Seconds: 0.69\n",
      "Epoch   1/50 Batch  220/485 - Loss:  2.548, Seconds: 0.73\n",
      "Epoch   1/50 Batch  225/485 - Loss:  2.718, Seconds: 0.61\n",
      "Epoch   1/50 Batch  230/485 - Loss:  2.593, Seconds: 0.73\n",
      "Epoch   1/50 Batch  235/485 - Loss:  2.627, Seconds: 0.74\n",
      "Epoch   1/50 Batch  240/485 - Loss:  2.962, Seconds: 0.61\n",
      "Epoch   1/50 Batch  245/485 - Loss:  2.468, Seconds: 0.72\n",
      "Epoch   1/50 Batch  250/485 - Loss:  2.490, Seconds: 0.84\n",
      "Epoch   1/50 Batch  255/485 - Loss:  2.628, Seconds: 0.83\n",
      "Epoch   1/50 Batch  260/485 - Loss:  2.858, Seconds: 0.76\n",
      "Epoch   1/50 Batch  265/485 - Loss:  2.694, Seconds: 0.66\n",
      "Epoch   1/50 Batch  270/485 - Loss:  2.676, Seconds: 0.74\n",
      "Epoch   1/50 Batch  275/485 - Loss:  2.532, Seconds: 0.80\n",
      "Epoch   1/50 Batch  280/485 - Loss:  2.560, Seconds: 0.82\n",
      "Epoch   1/50 Batch  285/485 - Loss:  2.746, Seconds: 0.77\n",
      "Epoch   1/50 Batch  290/485 - Loss:  2.489, Seconds: 0.81\n",
      "Epoch   1/50 Batch  295/485 - Loss:  2.381, Seconds: 0.87\n",
      "Epoch   1/50 Batch  300/485 - Loss:  2.398, Seconds: 0.82\n",
      "Epoch   1/50 Batch  305/485 - Loss:  2.659, Seconds: 0.84\n",
      "Epoch   1/50 Batch  310/485 - Loss:  2.685, Seconds: 0.76\n",
      "Epoch   1/50 Batch  315/485 - Loss:  2.723, Seconds: 0.79\n",
      "Epoch   1/50 Batch  320/485 - Loss:  2.748, Seconds: 0.85\n",
      "Average loss for this update: 2.615\n",
      "New Record!\n",
      "Epoch   1/50 Batch  325/485 - Loss:  2.906, Seconds: 0.74\n",
      "Epoch   1/50 Batch  330/485 - Loss:  2.473, Seconds: 0.95\n",
      "Epoch   1/50 Batch  335/485 - Loss:  2.762, Seconds: 0.88\n",
      "Epoch   1/50 Batch  340/485 - Loss:  2.794, Seconds: 0.85\n",
      "Epoch   1/50 Batch  345/485 - Loss:  2.850, Seconds: 0.87\n",
      "Epoch   1/50 Batch  350/485 - Loss:  2.839, Seconds: 0.87\n",
      "Epoch   1/50 Batch  355/485 - Loss:  2.737, Seconds: 0.92\n",
      "Epoch   1/50 Batch  360/485 - Loss:  2.573, Seconds: 0.98\n",
      "Epoch   1/50 Batch  365/485 - Loss:  2.885, Seconds: 1.01\n",
      "Epoch   1/50 Batch  370/485 - Loss:  2.918, Seconds: 0.95\n",
      "Epoch   1/50 Batch  375/485 - Loss:  2.902, Seconds: 0.93\n",
      "Epoch   1/50 Batch  380/485 - Loss:  2.716, Seconds: 0.98\n",
      "Epoch   1/50 Batch  385/485 - Loss:  2.657, Seconds: 0.94\n",
      "Epoch   1/50 Batch  390/485 - Loss:  2.552, Seconds: 1.01\n",
      "Epoch   1/50 Batch  395/485 - Loss:  2.849, Seconds: 0.98\n",
      "Epoch   1/50 Batch  400/485 - Loss:  2.850, Seconds: 1.04\n",
      "Epoch   1/50 Batch  405/485 - Loss:  2.736, Seconds: 1.07\n",
      "Epoch   1/50 Batch  410/485 - Loss:  2.669, Seconds: 1.11\n",
      "Epoch   1/50 Batch  415/485 - Loss:  2.900, Seconds: 1.19\n",
      "Epoch   1/50 Batch  420/485 - Loss:  2.977, Seconds: 1.12\n",
      "Epoch   1/50 Batch  425/485 - Loss:  2.774, Seconds: 1.12\n",
      "Epoch   1/50 Batch  430/485 - Loss:  2.826, Seconds: 1.22\n",
      "Epoch   1/50 Batch  435/485 - Loss:  2.960, Seconds: 1.21\n",
      "Epoch   1/50 Batch  440/485 - Loss:  3.042, Seconds: 1.26\n",
      "Epoch   1/50 Batch  445/485 - Loss:  2.924, Seconds: 1.28\n",
      "Epoch   1/50 Batch  450/485 - Loss:  2.941, Seconds: 1.28\n",
      "Epoch   1/50 Batch  455/485 - Loss:  2.799, Seconds: 1.43\n",
      "Epoch   1/50 Batch  460/485 - Loss:  3.234, Seconds: 1.40\n",
      "Epoch   1/50 Batch  465/485 - Loss:  3.021, Seconds: 1.54\n",
      "Epoch   1/50 Batch  470/485 - Loss:  3.172, Seconds: 1.66\n",
      "Epoch   1/50 Batch  475/485 - Loss:  3.133, Seconds: 1.86\n",
      "Epoch   1/50 Batch  480/485 - Loss:  3.249, Seconds: 1.93\n",
      "Average loss for this update: 2.863\n",
      "No Improvement.\n",
      "Epoch   2/50 Batch    5/485 - Loss:  2.706, Seconds: 0.49\n",
      "Epoch   2/50 Batch   10/485 - Loss:  2.082, Seconds: 0.53\n",
      "Epoch   2/50 Batch   15/485 - Loss:  2.078, Seconds: 0.47\n",
      "Epoch   2/50 Batch   20/485 - Loss:  1.931, Seconds: 0.65\n",
      "Epoch   2/50 Batch   25/485 - Loss:  2.175, Seconds: 0.49\n",
      "Epoch   2/50 Batch   30/485 - Loss:  2.027, Seconds: 0.64\n",
      "Epoch   2/50 Batch   35/485 - Loss:  2.148, Seconds: 0.63\n",
      "Epoch   2/50 Batch   40/485 - Loss:  2.197, Seconds: 0.50\n",
      "Epoch   2/50 Batch   45/485 - Loss:  2.141, Seconds: 0.59\n",
      "Epoch   2/50 Batch   50/485 - Loss:  2.121, Seconds: 0.69\n",
      "Epoch   2/50 Batch   55/485 - Loss:  2.169, Seconds: 0.44\n",
      "Epoch   2/50 Batch   60/485 - Loss:  2.158, Seconds: 0.44\n",
      "Epoch   2/50 Batch   65/485 - Loss:  2.054, Seconds: 0.50\n",
      "Epoch   2/50 Batch   70/485 - Loss:  1.954, Seconds: 0.55\n",
      "Epoch   2/50 Batch   75/485 - Loss:  2.239, Seconds: 0.59\n",
      "Epoch   2/50 Batch   80/485 - Loss:  2.197, Seconds: 0.58\n",
      "Epoch   2/50 Batch   85/485 - Loss:  2.135, Seconds: 0.59\n",
      "Epoch   2/50 Batch   90/485 - Loss:  2.164, Seconds: 0.57\n",
      "Epoch   2/50 Batch   95/485 - Loss:  2.110, Seconds: 0.59\n",
      "Epoch   2/50 Batch  100/485 - Loss:  1.946, Seconds: 0.70\n",
      "Epoch   2/50 Batch  105/485 - Loss:  2.042, Seconds: 0.70\n",
      "Epoch   2/50 Batch  110/485 - Loss:  2.063, Seconds: 0.65\n",
      "Epoch   2/50 Batch  115/485 - Loss:  2.213, Seconds: 0.65\n",
      "Epoch   2/50 Batch  120/485 - Loss:  2.050, Seconds: 0.62\n",
      "Epoch   2/50 Batch  125/485 - Loss:  2.207, Seconds: 0.58\n",
      "Epoch   2/50 Batch  130/485 - Loss:  2.074, Seconds: 0.54\n",
      "Epoch   2/50 Batch  135/485 - Loss:  2.059, Seconds: 0.66\n",
      "Epoch   2/50 Batch  140/485 - Loss:  2.280, Seconds: 0.62\n",
      "Epoch   2/50 Batch  145/485 - Loss:  2.032, Seconds: 0.66\n",
      "Epoch   2/50 Batch  150/485 - Loss:  2.184, Seconds: 0.60\n",
      "Epoch   2/50 Batch  155/485 - Loss:  2.138, Seconds: 0.65\n",
      "Epoch   2/50 Batch  160/485 - Loss:  1.938, Seconds: 0.58\n",
      "Average loss for this update: 2.125\n",
      "New Record!\n",
      "Epoch   2/50 Batch  165/485 - Loss:  2.169, Seconds: 0.59\n",
      "Epoch   2/50 Batch  170/485 - Loss:  2.186, Seconds: 0.63\n",
      "Epoch   2/50 Batch  175/485 - Loss:  1.941, Seconds: 0.69\n",
      "Epoch   2/50 Batch  180/485 - Loss:  2.078, Seconds: 0.70\n",
      "Epoch   2/50 Batch  185/485 - Loss:  2.178, Seconds: 0.62\n",
      "Epoch   2/50 Batch  190/485 - Loss:  2.163, Seconds: 0.75\n",
      "Epoch   2/50 Batch  195/485 - Loss:  2.158, Seconds: 0.63\n",
      "Epoch   2/50 Batch  200/485 - Loss:  2.306, Seconds: 0.62\n",
      "Epoch   2/50 Batch  205/485 - Loss:  2.226, Seconds: 0.59\n",
      "Epoch   2/50 Batch  210/485 - Loss:  2.255, Seconds: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/50 Batch  215/485 - Loss:  2.184, Seconds: 0.68\n",
      "Epoch   2/50 Batch  220/485 - Loss:  2.154, Seconds: 0.72\n",
      "Epoch   2/50 Batch  225/485 - Loss:  2.273, Seconds: 0.62\n",
      "Epoch   2/50 Batch  230/485 - Loss:  2.177, Seconds: 0.73\n",
      "Epoch   2/50 Batch  235/485 - Loss:  2.203, Seconds: 0.75\n",
      "Epoch   2/50 Batch  240/485 - Loss:  2.450, Seconds: 0.62\n",
      "Epoch   2/50 Batch  245/485 - Loss:  2.112, Seconds: 0.73\n",
      "Epoch   2/50 Batch  250/485 - Loss:  2.120, Seconds: 0.80\n",
      "Epoch   2/50 Batch  255/485 - Loss:  2.281, Seconds: 0.91\n",
      "Epoch   2/50 Batch  260/485 - Loss:  2.410, Seconds: 0.81\n",
      "Epoch   2/50 Batch  265/485 - Loss:  2.288, Seconds: 0.70\n",
      "Epoch   2/50 Batch  270/485 - Loss:  2.322, Seconds: 0.76\n",
      "Epoch   2/50 Batch  275/485 - Loss:  2.168, Seconds: 0.79\n",
      "Epoch   2/50 Batch  280/485 - Loss:  2.189, Seconds: 0.87\n",
      "Epoch   2/50 Batch  285/485 - Loss:  2.353, Seconds: 0.80\n",
      "Epoch   2/50 Batch  290/485 - Loss:  2.154, Seconds: 0.81\n",
      "Epoch   2/50 Batch  295/485 - Loss:  2.058, Seconds: 0.87\n",
      "Epoch   2/50 Batch  300/485 - Loss:  2.075, Seconds: 1.08\n",
      "Epoch   2/50 Batch  305/485 - Loss:  2.287, Seconds: 0.83\n",
      "Epoch   2/50 Batch  310/485 - Loss:  2.333, Seconds: 0.75\n",
      "Epoch   2/50 Batch  315/485 - Loss:  2.360, Seconds: 0.83\n",
      "Epoch   2/50 Batch  320/485 - Loss:  2.359, Seconds: 0.87\n",
      "Average loss for this update: 2.218\n",
      "No Improvement.\n",
      "Epoch   2/50 Batch  325/485 - Loss:  2.555, Seconds: 0.80\n",
      "Epoch   2/50 Batch  330/485 - Loss:  2.127, Seconds: 0.91\n",
      "Epoch   2/50 Batch  335/485 - Loss:  2.416, Seconds: 1.06\n",
      "Epoch   2/50 Batch  340/485 - Loss:  2.427, Seconds: 1.00\n",
      "Epoch   2/50 Batch  345/485 - Loss:  2.479, Seconds: 0.85\n",
      "Epoch   2/50 Batch  350/485 - Loss:  2.481, Seconds: 0.87\n",
      "Epoch   2/50 Batch  355/485 - Loss:  2.363, Seconds: 0.93\n",
      "Epoch   2/50 Batch  360/485 - Loss:  2.285, Seconds: 1.03\n",
      "Epoch   2/50 Batch  365/485 - Loss:  2.497, Seconds: 0.99\n",
      "Epoch   2/50 Batch  370/485 - Loss:  2.576, Seconds: 1.04\n",
      "Epoch   2/50 Batch  375/485 - Loss:  2.558, Seconds: 0.94\n",
      "Epoch   2/50 Batch  380/485 - Loss:  2.408, Seconds: 0.98\n",
      "Epoch   2/50 Batch  385/485 - Loss:  2.373, Seconds: 1.00\n",
      "Epoch   2/50 Batch  390/485 - Loss:  2.271, Seconds: 1.08\n",
      "Epoch   2/50 Batch  395/485 - Loss:  2.556, Seconds: 1.06\n",
      "Epoch   2/50 Batch  400/485 - Loss:  2.532, Seconds: 1.17\n",
      "Epoch   2/50 Batch  405/485 - Loss:  2.430, Seconds: 1.16\n",
      "Epoch   2/50 Batch  410/485 - Loss:  2.362, Seconds: 1.12\n",
      "Epoch   2/50 Batch  415/485 - Loss:  2.548, Seconds: 1.14\n",
      "Epoch   2/50 Batch  420/485 - Loss:  2.653, Seconds: 1.13\n",
      "Epoch   2/50 Batch  425/485 - Loss:  2.455, Seconds: 1.12\n",
      "Epoch   2/50 Batch  430/485 - Loss:  2.538, Seconds: 1.24\n",
      "Epoch   2/50 Batch  435/485 - Loss:  2.623, Seconds: 1.25\n",
      "Epoch   2/50 Batch  440/485 - Loss:  2.708, Seconds: 1.31\n",
      "Epoch   2/50 Batch  445/485 - Loss:  2.622, Seconds: 1.29\n",
      "Epoch   2/50 Batch  450/485 - Loss:  2.623, Seconds: 1.27\n",
      "Epoch   2/50 Batch  455/485 - Loss:  2.515, Seconds: 1.36\n",
      "Epoch   2/50 Batch  460/485 - Loss:  2.875, Seconds: 1.40\n",
      "Epoch   2/50 Batch  465/485 - Loss:  2.723, Seconds: 1.57\n",
      "Epoch   2/50 Batch  470/485 - Loss:  2.840, Seconds: 1.66\n",
      "Epoch   2/50 Batch  475/485 - Loss:  2.824, Seconds: 1.83\n",
      "Epoch   2/50 Batch  480/485 - Loss:  2.913, Seconds: 1.89\n",
      "Average loss for this update: 2.536\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch    5/485 - Loss:  2.538, Seconds: 0.49\n",
      "Epoch   3/50 Batch   10/485 - Loss:  1.915, Seconds: 0.54\n",
      "Epoch   3/50 Batch   15/485 - Loss:  1.874, Seconds: 0.53\n",
      "Epoch   3/50 Batch   20/485 - Loss:  1.787, Seconds: 0.59\n",
      "Epoch   3/50 Batch   25/485 - Loss:  2.011, Seconds: 0.44\n",
      "Epoch   3/50 Batch   30/485 - Loss:  1.852, Seconds: 0.61\n",
      "Epoch   3/50 Batch   35/485 - Loss:  1.952, Seconds: 0.56\n",
      "Epoch   3/50 Batch   40/485 - Loss:  1.990, Seconds: 0.46\n",
      "Epoch   3/50 Batch   45/485 - Loss:  1.977, Seconds: 0.58\n",
      "Epoch   3/50 Batch   50/485 - Loss:  1.915, Seconds: 0.63\n",
      "Epoch   3/50 Batch   55/485 - Loss:  1.942, Seconds: 0.44\n",
      "Epoch   3/50 Batch   60/485 - Loss:  1.984, Seconds: 0.43\n",
      "Epoch   3/50 Batch   65/485 - Loss:  1.844, Seconds: 0.51\n",
      "Epoch   3/50 Batch   70/485 - Loss:  1.753, Seconds: 0.54\n",
      "Epoch   3/50 Batch   75/485 - Loss:  2.042, Seconds: 0.59\n",
      "Epoch   3/50 Batch   80/485 - Loss:  1.933, Seconds: 0.61\n",
      "Epoch   3/50 Batch   85/485 - Loss:  1.947, Seconds: 0.58\n",
      "Epoch   3/50 Batch   90/485 - Loss:  1.932, Seconds: 0.53\n",
      "Epoch   3/50 Batch   95/485 - Loss:  1.929, Seconds: 0.58\n",
      "Epoch   3/50 Batch  100/485 - Loss:  1.759, Seconds: 0.70\n",
      "Epoch   3/50 Batch  105/485 - Loss:  1.839, Seconds: 0.72\n",
      "Epoch   3/50 Batch  110/485 - Loss:  1.908, Seconds: 0.63\n",
      "Epoch   3/50 Batch  115/485 - Loss:  2.020, Seconds: 0.68\n",
      "Epoch   3/50 Batch  120/485 - Loss:  1.871, Seconds: 0.64\n",
      "Epoch   3/50 Batch  125/485 - Loss:  2.026, Seconds: 0.59\n",
      "Epoch   3/50 Batch  130/485 - Loss:  1.892, Seconds: 0.55\n",
      "Epoch   3/50 Batch  135/485 - Loss:  1.881, Seconds: 0.68\n",
      "Epoch   3/50 Batch  140/485 - Loss:  2.107, Seconds: 0.59\n",
      "Epoch   3/50 Batch  145/485 - Loss:  1.876, Seconds: 0.65\n",
      "Epoch   3/50 Batch  150/485 - Loss:  1.973, Seconds: 0.60\n",
      "Epoch   3/50 Batch  155/485 - Loss:  1.975, Seconds: 0.64\n",
      "Epoch   3/50 Batch  160/485 - Loss:  1.784, Seconds: 0.58\n",
      "Average loss for this update: 1.938\n",
      "New Record!\n",
      "Epoch   3/50 Batch  165/485 - Loss:  1.980, Seconds: 0.63\n",
      "Epoch   3/50 Batch  170/485 - Loss:  1.984, Seconds: 0.67\n",
      "Epoch   3/50 Batch  175/485 - Loss:  1.779, Seconds: 0.73\n",
      "Epoch   3/50 Batch  180/485 - Loss:  1.906, Seconds: 0.73\n",
      "Epoch   3/50 Batch  185/485 - Loss:  1.974, Seconds: 0.64\n",
      "Epoch   3/50 Batch  190/485 - Loss:  1.975, Seconds: 0.85\n",
      "Epoch   3/50 Batch  195/485 - Loss:  1.974, Seconds: 0.68\n",
      "Epoch   3/50 Batch  200/485 - Loss:  2.111, Seconds: 0.68\n",
      "Epoch   3/50 Batch  205/485 - Loss:  2.053, Seconds: 0.63\n",
      "Epoch   3/50 Batch  210/485 - Loss:  2.062, Seconds: 0.77\n",
      "Epoch   3/50 Batch  215/485 - Loss:  2.004, Seconds: 0.76\n",
      "Epoch   3/50 Batch  220/485 - Loss:  1.987, Seconds: 0.74\n",
      "Epoch   3/50 Batch  225/485 - Loss:  2.065, Seconds: 0.63\n",
      "Epoch   3/50 Batch  230/485 - Loss:  1.983, Seconds: 0.74\n",
      "Epoch   3/50 Batch  235/485 - Loss:  2.007, Seconds: 0.74\n",
      "Epoch   3/50 Batch  240/485 - Loss:  2.233, Seconds: 0.58\n",
      "Epoch   3/50 Batch  245/485 - Loss:  1.936, Seconds: 0.73\n",
      "Epoch   3/50 Batch  250/485 - Loss:  1.957, Seconds: 0.82\n",
      "Epoch   3/50 Batch  255/485 - Loss:  2.108, Seconds: 0.83\n",
      "Epoch   3/50 Batch  260/485 - Loss:  2.201, Seconds: 0.78\n",
      "Epoch   3/50 Batch  265/485 - Loss:  2.061, Seconds: 0.67\n",
      "Epoch   3/50 Batch  270/485 - Loss:  2.129, Seconds: 0.75\n",
      "Epoch   3/50 Batch  275/485 - Loss:  2.009, Seconds: 0.78\n",
      "Epoch   3/50 Batch  280/485 - Loss:  2.027, Seconds: 0.81\n",
      "Epoch   3/50 Batch  285/485 - Loss:  2.160, Seconds: 0.78\n",
      "Epoch   3/50 Batch  290/485 - Loss:  1.979, Seconds: 0.82\n",
      "Epoch   3/50 Batch  295/485 - Loss:  1.898, Seconds: 0.89\n",
      "Epoch   3/50 Batch  300/485 - Loss:  1.919, Seconds: 0.87\n",
      "Epoch   3/50 Batch  305/485 - Loss:  2.078, Seconds: 0.84\n",
      "Epoch   3/50 Batch  310/485 - Loss:  2.150, Seconds: 0.79\n",
      "Epoch   3/50 Batch  315/485 - Loss:  2.183, Seconds: 0.81\n",
      "Epoch   3/50 Batch  320/485 - Loss:  2.174, Seconds: 0.86\n",
      "Average loss for this update: 2.033\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  325/485 - Loss:  2.338, Seconds: 0.75\n",
      "Epoch   3/50 Batch  330/485 - Loss:  1.974, Seconds: 0.91\n",
      "Epoch   3/50 Batch  335/485 - Loss:  2.227, Seconds: 0.91\n",
      "Epoch   3/50 Batch  340/485 - Loss:  2.229, Seconds: 1.01\n",
      "Epoch   3/50 Batch  345/485 - Loss:  2.283, Seconds: 1.02\n",
      "Epoch   3/50 Batch  350/485 - Loss:  2.302, Seconds: 0.87\n",
      "Epoch   3/50 Batch  355/485 - Loss:  2.177, Seconds: 0.96\n",
      "Epoch   3/50 Batch  360/485 - Loss:  2.124, Seconds: 1.15\n",
      "Epoch   3/50 Batch  365/485 - Loss:  2.250, Seconds: 1.11\n",
      "Epoch   3/50 Batch  370/485 - Loss:  2.384, Seconds: 0.98\n",
      "Epoch   3/50 Batch  375/485 - Loss:  2.358, Seconds: 0.95\n",
      "Epoch   3/50 Batch  380/485 - Loss:  2.233, Seconds: 1.05\n",
      "Epoch   3/50 Batch  385/485 - Loss:  2.213, Seconds: 0.98\n",
      "Epoch   3/50 Batch  390/485 - Loss:  2.084, Seconds: 1.05\n",
      "Epoch   3/50 Batch  395/485 - Loss:  2.373, Seconds: 0.98\n",
      "Epoch   3/50 Batch  400/485 - Loss:  2.345, Seconds: 1.07\n",
      "Epoch   3/50 Batch  405/485 - Loss:  2.236, Seconds: 1.08\n",
      "Epoch   3/50 Batch  410/485 - Loss:  2.194, Seconds: 1.09\n",
      "Epoch   3/50 Batch  415/485 - Loss:  2.384, Seconds: 1.16\n",
      "Epoch   3/50 Batch  420/485 - Loss:  2.450, Seconds: 1.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/50 Batch  425/485 - Loss:  2.274, Seconds: 1.12\n",
      "Epoch   3/50 Batch  430/485 - Loss:  2.371, Seconds: 1.20\n",
      "Epoch   3/50 Batch  435/485 - Loss:  2.474, Seconds: 1.24\n",
      "Epoch   3/50 Batch  440/485 - Loss:  2.513, Seconds: 1.29\n",
      "Epoch   3/50 Batch  445/485 - Loss:  2.453, Seconds: 1.28\n",
      "Epoch   3/50 Batch  450/485 - Loss:  2.467, Seconds: 1.31\n",
      "Epoch   3/50 Batch  455/485 - Loss:  2.367, Seconds: 1.38\n",
      "Epoch   3/50 Batch  460/485 - Loss:  2.679, Seconds: 1.43\n",
      "Epoch   3/50 Batch  465/485 - Loss:  2.521, Seconds: 1.55\n",
      "Epoch   3/50 Batch  470/485 - Loss:  2.669, Seconds: 1.69\n",
      "Epoch   3/50 Batch  475/485 - Loss:  2.615, Seconds: 1.83\n",
      "Epoch   3/50 Batch  480/485 - Loss:  2.676, Seconds: 1.90\n",
      "Average loss for this update: 2.351\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch    5/485 - Loss:  2.382, Seconds: 0.50\n",
      "Epoch   4/50 Batch   10/485 - Loss:  1.798, Seconds: 0.54\n",
      "Epoch   4/50 Batch   15/485 - Loss:  1.759, Seconds: 0.51\n",
      "Epoch   4/50 Batch   20/485 - Loss:  1.644, Seconds: 0.60\n",
      "Epoch   4/50 Batch   25/485 - Loss:  1.854, Seconds: 0.44\n",
      "Epoch   4/50 Batch   30/485 - Loss:  1.761, Seconds: 0.62\n",
      "Epoch   4/50 Batch   35/485 - Loss:  1.855, Seconds: 0.56\n",
      "Epoch   4/50 Batch   40/485 - Loss:  1.878, Seconds: 0.45\n",
      "Epoch   4/50 Batch   45/485 - Loss:  1.845, Seconds: 0.58\n",
      "Epoch   4/50 Batch   50/485 - Loss:  1.859, Seconds: 0.60\n",
      "Epoch   4/50 Batch   55/485 - Loss:  1.843, Seconds: 0.44\n",
      "Epoch   4/50 Batch   60/485 - Loss:  1.850, Seconds: 0.43\n",
      "Epoch   4/50 Batch   65/485 - Loss:  1.791, Seconds: 0.52\n",
      "Epoch   4/50 Batch   70/485 - Loss:  1.673, Seconds: 0.55\n",
      "Epoch   4/50 Batch   75/485 - Loss:  1.941, Seconds: 0.61\n",
      "Epoch   4/50 Batch   80/485 - Loss:  1.839, Seconds: 0.58\n",
      "Epoch   4/50 Batch   85/485 - Loss:  1.812, Seconds: 0.58\n",
      "Epoch   4/50 Batch   90/485 - Loss:  1.831, Seconds: 0.53\n",
      "Epoch   4/50 Batch   95/485 - Loss:  1.813, Seconds: 0.56\n",
      "Epoch   4/50 Batch  100/485 - Loss:  1.674, Seconds: 0.70\n",
      "Epoch   4/50 Batch  105/485 - Loss:  1.756, Seconds: 0.69\n",
      "Epoch   4/50 Batch  110/485 - Loss:  1.779, Seconds: 0.62\n",
      "Epoch   4/50 Batch  115/485 - Loss:  1.916, Seconds: 0.66\n",
      "Epoch   4/50 Batch  120/485 - Loss:  1.780, Seconds: 0.63\n",
      "Epoch   4/50 Batch  125/485 - Loss:  1.905, Seconds: 0.59\n",
      "Epoch   4/50 Batch  130/485 - Loss:  1.771, Seconds: 0.57\n",
      "Epoch   4/50 Batch  135/485 - Loss:  1.801, Seconds: 0.68\n",
      "Epoch   4/50 Batch  140/485 - Loss:  1.973, Seconds: 0.60\n",
      "Epoch   4/50 Batch  145/485 - Loss:  1.781, Seconds: 0.64\n",
      "Epoch   4/50 Batch  150/485 - Loss:  1.891, Seconds: 0.63\n",
      "Epoch   4/50 Batch  155/485 - Loss:  1.827, Seconds: 0.64\n",
      "Epoch   4/50 Batch  160/485 - Loss:  1.691, Seconds: 0.59\n",
      "Average loss for this update: 1.83\n",
      "New Record!\n",
      "Epoch   4/50 Batch  165/485 - Loss:  1.889, Seconds: 0.61\n",
      "Epoch   4/50 Batch  170/485 - Loss:  1.883, Seconds: 0.68\n",
      "Epoch   4/50 Batch  175/485 - Loss:  1.671, Seconds: 0.71\n",
      "Epoch   4/50 Batch  180/485 - Loss:  1.795, Seconds: 0.70\n",
      "Epoch   4/50 Batch  185/485 - Loss:  1.882, Seconds: 0.63\n",
      "Epoch   4/50 Batch  190/485 - Loss:  1.871, Seconds: 0.74\n",
      "Epoch   4/50 Batch  195/485 - Loss:  1.861, Seconds: 0.63\n",
      "Epoch   4/50 Batch  200/485 - Loss:  1.987, Seconds: 0.66\n",
      "Epoch   4/50 Batch  205/485 - Loss:  1.930, Seconds: 0.60\n",
      "Epoch   4/50 Batch  210/485 - Loss:  1.938, Seconds: 0.78\n",
      "Epoch   4/50 Batch  215/485 - Loss:  1.862, Seconds: 0.70\n",
      "Epoch   4/50 Batch  220/485 - Loss:  1.848, Seconds: 0.72\n",
      "Epoch   4/50 Batch  225/485 - Loss:  1.942, Seconds: 0.61\n",
      "Epoch   4/50 Batch  230/485 - Loss:  1.863, Seconds: 0.75\n",
      "Epoch   4/50 Batch  235/485 - Loss:  1.888, Seconds: 0.78\n",
      "Epoch   4/50 Batch  240/485 - Loss:  2.065, Seconds: 0.60\n",
      "Epoch   4/50 Batch  245/485 - Loss:  1.846, Seconds: 0.75\n",
      "Epoch   4/50 Batch  250/485 - Loss:  1.874, Seconds: 0.82\n",
      "Epoch   4/50 Batch  255/485 - Loss:  2.009, Seconds: 0.81\n",
      "Epoch   4/50 Batch  260/485 - Loss:  2.092, Seconds: 0.80\n",
      "Epoch   4/50 Batch  265/485 - Loss:  1.940, Seconds: 0.68\n",
      "Epoch   4/50 Batch  270/485 - Loss:  1.996, Seconds: 0.77\n",
      "Epoch   4/50 Batch  275/485 - Loss:  1.891, Seconds: 0.82\n",
      "Epoch   4/50 Batch  280/485 - Loss:  1.888, Seconds: 0.81\n",
      "Epoch   4/50 Batch  285/485 - Loss:  2.035, Seconds: 0.75\n",
      "Epoch   4/50 Batch  290/485 - Loss:  1.866, Seconds: 0.81\n",
      "Epoch   4/50 Batch  295/485 - Loss:  1.799, Seconds: 0.86\n",
      "Epoch   4/50 Batch  300/485 - Loss:  1.833, Seconds: 0.86\n",
      "Epoch   4/50 Batch  305/485 - Loss:  1.957, Seconds: 0.86\n",
      "Epoch   4/50 Batch  310/485 - Loss:  2.059, Seconds: 0.78\n",
      "Epoch   4/50 Batch  315/485 - Loss:  2.074, Seconds: 0.82\n",
      "Epoch   4/50 Batch  320/485 - Loss:  2.074, Seconds: 0.88\n",
      "Average loss for this update: 1.919\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  325/485 - Loss:  2.195, Seconds: 0.77\n",
      "Epoch   4/50 Batch  330/485 - Loss:  1.853, Seconds: 0.93\n",
      "Epoch   4/50 Batch  335/485 - Loss:  2.105, Seconds: 0.92\n",
      "Epoch   4/50 Batch  340/485 - Loss:  2.107, Seconds: 0.86\n",
      "Epoch   4/50 Batch  345/485 - Loss:  2.120, Seconds: 0.86\n",
      "Epoch   4/50 Batch  350/485 - Loss:  2.185, Seconds: 0.89\n",
      "Epoch   4/50 Batch  355/485 - Loss:  2.038, Seconds: 0.91\n",
      "Epoch   4/50 Batch  360/485 - Loss:  2.004, Seconds: 1.01\n",
      "Epoch   4/50 Batch  365/485 - Loss:  2.165, Seconds: 1.01\n",
      "Epoch   4/50 Batch  370/485 - Loss:  2.212, Seconds: 0.99\n",
      "Epoch   4/50 Batch  375/485 - Loss:  2.205, Seconds: 0.92\n",
      "Epoch   4/50 Batch  380/485 - Loss:  2.138, Seconds: 0.99\n",
      "Epoch   4/50 Batch  385/485 - Loss:  2.137, Seconds: 0.97\n",
      "Epoch   4/50 Batch  390/485 - Loss:  1.967, Seconds: 1.04\n",
      "Epoch   4/50 Batch  395/485 - Loss:  2.230, Seconds: 0.99\n",
      "Epoch   4/50 Batch  400/485 - Loss:  2.217, Seconds: 1.10\n",
      "Epoch   4/50 Batch  405/485 - Loss:  2.145, Seconds: 2.82\n",
      "Epoch   4/50 Batch  410/485 - Loss:  2.114, Seconds: 1.07\n",
      "Epoch   4/50 Batch  415/485 - Loss:  2.273, Seconds: 1.17\n",
      "Epoch   4/50 Batch  420/485 - Loss:  2.322, Seconds: 1.11\n",
      "Epoch   4/50 Batch  425/485 - Loss:  2.136, Seconds: 1.09\n",
      "Epoch   4/50 Batch  430/485 - Loss:  2.240, Seconds: 1.19\n",
      "Epoch   4/50 Batch  435/485 - Loss:  2.332, Seconds: 1.19\n",
      "Epoch   4/50 Batch  440/485 - Loss:  2.408, Seconds: 1.27\n",
      "Epoch   4/50 Batch  445/485 - Loss:  2.323, Seconds: 1.31\n",
      "Epoch   4/50 Batch  450/485 - Loss:  2.323, Seconds: 1.25\n",
      "Epoch   4/50 Batch  455/485 - Loss:  2.265, Seconds: 1.36\n",
      "Epoch   4/50 Batch  460/485 - Loss:  2.538, Seconds: 1.40\n",
      "Epoch   4/50 Batch  465/485 - Loss:  2.407, Seconds: 1.56\n",
      "Epoch   4/50 Batch  470/485 - Loss:  2.519, Seconds: 1.68\n",
      "Epoch   4/50 Batch  475/485 - Loss:  2.517, Seconds: 1.86\n",
      "Epoch   4/50 Batch  480/485 - Loss:  2.587, Seconds: 1.86\n",
      "Average loss for this update: 2.229\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch    5/485 - Loss:  2.256, Seconds: 0.49\n",
      "Epoch   5/50 Batch   10/485 - Loss:  1.690, Seconds: 0.55\n",
      "Epoch   5/50 Batch   15/485 - Loss:  1.702, Seconds: 0.48\n",
      "Epoch   5/50 Batch   20/485 - Loss:  1.578, Seconds: 0.61\n",
      "Epoch   5/50 Batch   25/485 - Loss:  1.747, Seconds: 0.44\n",
      "Epoch   5/50 Batch   30/485 - Loss:  1.649, Seconds: 0.62\n",
      "Epoch   5/50 Batch   35/485 - Loss:  1.765, Seconds: 0.58\n",
      "Epoch   5/50 Batch   40/485 - Loss:  1.761, Seconds: 0.46\n",
      "Epoch   5/50 Batch   45/485 - Loss:  1.749, Seconds: 0.58\n",
      "Epoch   5/50 Batch   50/485 - Loss:  1.739, Seconds: 0.61\n",
      "Epoch   5/50 Batch   55/485 - Loss:  1.737, Seconds: 0.44\n",
      "Epoch   5/50 Batch   60/485 - Loss:  1.708, Seconds: 0.43\n",
      "Epoch   5/50 Batch   65/485 - Loss:  1.664, Seconds: 0.51\n",
      "Epoch   5/50 Batch   70/485 - Loss:  1.571, Seconds: 0.56\n",
      "Epoch   5/50 Batch   75/485 - Loss:  1.802, Seconds: 0.59\n",
      "Epoch   5/50 Batch   80/485 - Loss:  1.715, Seconds: 0.59\n",
      "Epoch   5/50 Batch   85/485 - Loss:  1.713, Seconds: 0.55\n",
      "Epoch   5/50 Batch   90/485 - Loss:  1.737, Seconds: 0.52\n",
      "Epoch   5/50 Batch   95/485 - Loss:  1.714, Seconds: 0.56\n",
      "Epoch   5/50 Batch  100/485 - Loss:  1.569, Seconds: 0.69\n",
      "Epoch   5/50 Batch  105/485 - Loss:  1.638, Seconds: 0.70\n",
      "Epoch   5/50 Batch  110/485 - Loss:  1.674, Seconds: 0.61\n",
      "Epoch   5/50 Batch  115/485 - Loss:  1.791, Seconds: 0.66\n",
      "Epoch   5/50 Batch  120/485 - Loss:  1.681, Seconds: 0.63\n",
      "Epoch   5/50 Batch  125/485 - Loss:  1.808, Seconds: 0.59\n",
      "Epoch   5/50 Batch  130/485 - Loss:  1.664, Seconds: 0.54\n",
      "Epoch   5/50 Batch  135/485 - Loss:  1.724, Seconds: 0.70\n",
      "Epoch   5/50 Batch  140/485 - Loss:  1.859, Seconds: 0.61\n",
      "Epoch   5/50 Batch  145/485 - Loss:  1.664, Seconds: 0.64\n",
      "Epoch   5/50 Batch  150/485 - Loss:  1.777, Seconds: 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 Batch  155/485 - Loss:  1.744, Seconds: 0.67\n",
      "Epoch   5/50 Batch  160/485 - Loss:  1.589, Seconds: 0.59\n",
      "Average loss for this update: 1.724\n",
      "New Record!\n",
      "Epoch   5/50 Batch  165/485 - Loss:  1.772, Seconds: 0.61\n",
      "Epoch   5/50 Batch  170/485 - Loss:  1.783, Seconds: 0.64\n",
      "Epoch   5/50 Batch  175/485 - Loss:  1.592, Seconds: 0.71\n",
      "Epoch   5/50 Batch  180/485 - Loss:  1.695, Seconds: 0.80\n",
      "Epoch   5/50 Batch  185/485 - Loss:  1.771, Seconds: 0.65\n",
      "Epoch   5/50 Batch  190/485 - Loss:  1.743, Seconds: 0.78\n",
      "Epoch   5/50 Batch  195/485 - Loss:  1.760, Seconds: 0.65\n",
      "Epoch   5/50 Batch  200/485 - Loss:  1.865, Seconds: 0.65\n",
      "Epoch   5/50 Batch  205/485 - Loss:  1.828, Seconds: 0.62\n",
      "Epoch   5/50 Batch  210/485 - Loss:  1.837, Seconds: 0.81\n",
      "Epoch   5/50 Batch  215/485 - Loss:  1.785, Seconds: 0.71\n",
      "Epoch   5/50 Batch  220/485 - Loss:  1.771, Seconds: 0.80\n",
      "Epoch   5/50 Batch  225/485 - Loss:  1.846, Seconds: 0.63\n",
      "Epoch   5/50 Batch  230/485 - Loss:  1.772, Seconds: 0.86\n",
      "Epoch   5/50 Batch  235/485 - Loss:  1.769, Seconds: 0.77\n",
      "Epoch   5/50 Batch  240/485 - Loss:  1.928, Seconds: 0.68\n",
      "Epoch   5/50 Batch  245/485 - Loss:  1.759, Seconds: 0.75\n",
      "Epoch   5/50 Batch  250/485 - Loss:  1.764, Seconds: 0.85\n",
      "Epoch   5/50 Batch  255/485 - Loss:  1.900, Seconds: 0.82\n",
      "Epoch   5/50 Batch  260/485 - Loss:  1.959, Seconds: 0.81\n",
      "Epoch   5/50 Batch  265/485 - Loss:  1.838, Seconds: 0.65\n",
      "Epoch   5/50 Batch  270/485 - Loss:  1.910, Seconds: 0.75\n",
      "Epoch   5/50 Batch  275/485 - Loss:  1.784, Seconds: 0.81\n",
      "Epoch   5/50 Batch  280/485 - Loss:  1.789, Seconds: 0.84\n",
      "Epoch   5/50 Batch  285/485 - Loss:  1.947, Seconds: 0.78\n",
      "Epoch   5/50 Batch  290/485 - Loss:  1.787, Seconds: 0.87\n",
      "Epoch   5/50 Batch  295/485 - Loss:  1.718, Seconds: 0.87\n",
      "Epoch   5/50 Batch  300/485 - Loss:  1.749, Seconds: 0.87\n",
      "Epoch   5/50 Batch  305/485 - Loss:  1.887, Seconds: 0.86\n",
      "Epoch   5/50 Batch  310/485 - Loss:  1.915, Seconds: 0.75\n",
      "Epoch   5/50 Batch  315/485 - Loss:  1.971, Seconds: 0.83\n",
      "Epoch   5/50 Batch  320/485 - Loss:  2.008, Seconds: 0.87\n",
      "Average loss for this update: 1.819\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  325/485 - Loss:  2.083, Seconds: 0.76\n",
      "Epoch   5/50 Batch  330/485 - Loss:  1.760, Seconds: 0.94\n",
      "Epoch   5/50 Batch  335/485 - Loss:  2.019, Seconds: 0.92\n",
      "Epoch   5/50 Batch  340/485 - Loss:  2.006, Seconds: 0.87\n",
      "Epoch   5/50 Batch  345/485 - Loss:  2.026, Seconds: 0.89\n",
      "Epoch   5/50 Batch  350/485 - Loss:  2.094, Seconds: 0.89\n",
      "Epoch   5/50 Batch  355/485 - Loss:  1.938, Seconds: 0.96\n",
      "Epoch   5/50 Batch  360/485 - Loss:  1.934, Seconds: 1.03\n",
      "Epoch   5/50 Batch  365/485 - Loss:  2.055, Seconds: 1.01\n",
      "Epoch   5/50 Batch  370/485 - Loss:  2.119, Seconds: 1.01\n",
      "Epoch   5/50 Batch  375/485 - Loss:  2.096, Seconds: 0.95\n",
      "Epoch   5/50 Batch  380/485 - Loss:  2.037, Seconds: 1.03\n",
      "Epoch   5/50 Batch  385/485 - Loss:  2.066, Seconds: 1.13\n",
      "Epoch   5/50 Batch  390/485 - Loss:  1.873, Seconds: 1.11\n",
      "Epoch   5/50 Batch  395/485 - Loss:  2.136, Seconds: 1.11\n",
      "Epoch   5/50 Batch  400/485 - Loss:  2.118, Seconds: 1.16\n",
      "Epoch   5/50 Batch  405/485 - Loss:  2.021, Seconds: 1.07\n",
      "Epoch   5/50 Batch  410/485 - Loss:  2.010, Seconds: 1.12\n",
      "Epoch   5/50 Batch  415/485 - Loss:  2.173, Seconds: 1.21\n",
      "Epoch   5/50 Batch  420/485 - Loss:  2.216, Seconds: 1.28\n",
      "Epoch   5/50 Batch  425/485 - Loss:  2.058, Seconds: 1.14\n",
      "Epoch   5/50 Batch  430/485 - Loss:  2.166, Seconds: 1.33\n",
      "Epoch   5/50 Batch  435/485 - Loss:  2.253, Seconds: 1.23\n",
      "Epoch   5/50 Batch  440/485 - Loss:  2.275, Seconds: 1.29\n",
      "Epoch   5/50 Batch  445/485 - Loss:  2.260, Seconds: 1.31\n",
      "Epoch   5/50 Batch  450/485 - Loss:  2.248, Seconds: 1.29\n",
      "Epoch   5/50 Batch  455/485 - Loss:  2.136, Seconds: 1.40\n",
      "Epoch   5/50 Batch  460/485 - Loss:  2.461, Seconds: 1.46\n",
      "Epoch   5/50 Batch  465/485 - Loss:  2.324, Seconds: 1.70\n",
      "Epoch   5/50 Batch  470/485 - Loss:  2.438, Seconds: 1.73\n",
      "Epoch   5/50 Batch  475/485 - Loss:  2.418, Seconds: 1.94\n",
      "Epoch   5/50 Batch  480/485 - Loss:  2.463, Seconds: 2.08\n",
      "Average loss for this update: 2.134\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch    5/485 - Loss:  2.247, Seconds: 0.49\n",
      "Epoch   6/50 Batch   10/485 - Loss:  1.577, Seconds: 0.55\n",
      "Epoch   6/50 Batch   15/485 - Loss:  1.611, Seconds: 0.48\n",
      "Epoch   6/50 Batch   20/485 - Loss:  1.526, Seconds: 0.59\n",
      "Epoch   6/50 Batch   25/485 - Loss:  1.681, Seconds: 0.44\n",
      "Epoch   6/50 Batch   30/485 - Loss:  1.550, Seconds: 0.63\n",
      "Epoch   6/50 Batch   35/485 - Loss:  1.659, Seconds: 0.57\n",
      "Epoch   6/50 Batch   40/485 - Loss:  1.670, Seconds: 0.46\n",
      "Epoch   6/50 Batch   45/485 - Loss:  1.651, Seconds: 0.59\n",
      "Epoch   6/50 Batch   50/485 - Loss:  1.674, Seconds: 0.62\n",
      "Epoch   6/50 Batch   55/485 - Loss:  1.634, Seconds: 0.43\n",
      "Epoch   6/50 Batch   60/485 - Loss:  1.645, Seconds: 0.45\n",
      "Epoch   6/50 Batch   65/485 - Loss:  1.583, Seconds: 0.54\n",
      "Epoch   6/50 Batch   70/485 - Loss:  1.486, Seconds: 0.61\n",
      "Epoch   6/50 Batch   75/485 - Loss:  1.711, Seconds: 0.62\n",
      "Epoch   6/50 Batch   80/485 - Loss:  1.618, Seconds: 0.62\n",
      "Epoch   6/50 Batch   85/485 - Loss:  1.607, Seconds: 0.57\n",
      "Epoch   6/50 Batch   90/485 - Loss:  1.625, Seconds: 0.53\n",
      "Epoch   6/50 Batch   95/485 - Loss:  1.587, Seconds: 0.57\n",
      "Epoch   6/50 Batch  100/485 - Loss:  1.486, Seconds: 0.69\n",
      "Epoch   6/50 Batch  105/485 - Loss:  1.601, Seconds: 0.71\n",
      "Epoch   6/50 Batch  110/485 - Loss:  1.601, Seconds: 0.62\n",
      "Epoch   6/50 Batch  115/485 - Loss:  1.731, Seconds: 0.70\n",
      "Epoch   6/50 Batch  120/485 - Loss:  1.576, Seconds: 0.66\n",
      "Epoch   6/50 Batch  125/485 - Loss:  1.711, Seconds: 0.62\n",
      "Epoch   6/50 Batch  130/485 - Loss:  1.587, Seconds: 0.54\n",
      "Epoch   6/50 Batch  135/485 - Loss:  1.633, Seconds: 0.70\n",
      "Epoch   6/50 Batch  140/485 - Loss:  1.813, Seconds: 0.61\n",
      "Epoch   6/50 Batch  145/485 - Loss:  1.592, Seconds: 0.64\n",
      "Epoch   6/50 Batch  150/485 - Loss:  1.687, Seconds: 0.61\n",
      "Epoch   6/50 Batch  155/485 - Loss:  1.664, Seconds: 0.65\n",
      "Epoch   6/50 Batch  160/485 - Loss:  1.549, Seconds: 0.55\n",
      "Average loss for this update: 1.643\n",
      "New Record!\n",
      "Epoch   6/50 Batch  165/485 - Loss:  1.712, Seconds: 0.62\n",
      "Epoch   6/50 Batch  170/485 - Loss:  1.691, Seconds: 0.64\n",
      "Epoch   6/50 Batch  175/485 - Loss:  1.539, Seconds: 0.70\n",
      "Epoch   6/50 Batch  180/485 - Loss:  1.624, Seconds: 0.71\n",
      "Epoch   6/50 Batch  185/485 - Loss:  1.692, Seconds: 0.63\n",
      "Epoch   6/50 Batch  190/485 - Loss:  1.679, Seconds: 0.74\n",
      "Epoch   6/50 Batch  195/485 - Loss:  1.696, Seconds: 0.65\n",
      "Epoch   6/50 Batch  200/485 - Loss:  1.804, Seconds: 0.64\n",
      "Epoch   6/50 Batch  205/485 - Loss:  1.767, Seconds: 0.65\n",
      "Epoch   6/50 Batch  210/485 - Loss:  1.741, Seconds: 0.79\n",
      "Epoch   6/50 Batch  215/485 - Loss:  1.692, Seconds: 0.71\n",
      "Epoch   6/50 Batch  220/485 - Loss:  1.703, Seconds: 0.74\n",
      "Epoch   6/50 Batch  225/485 - Loss:  1.764, Seconds: 0.67\n",
      "Epoch   6/50 Batch  230/485 - Loss:  1.715, Seconds: 0.79\n",
      "Epoch   6/50 Batch  235/485 - Loss:  1.698, Seconds: 0.81\n",
      "Epoch   6/50 Batch  240/485 - Loss:  1.836, Seconds: 0.77\n",
      "Epoch   6/50 Batch  245/485 - Loss:  1.678, Seconds: 0.83\n",
      "Epoch   6/50 Batch  250/485 - Loss:  1.680, Seconds: 0.89\n",
      "Epoch   6/50 Batch  255/485 - Loss:  1.820, Seconds: 0.88\n",
      "Epoch   6/50 Batch  260/485 - Loss:  1.850, Seconds: 0.84\n",
      "Epoch   6/50 Batch  265/485 - Loss:  1.733, Seconds: 0.73\n",
      "Epoch   6/50 Batch  270/485 - Loss:  1.827, Seconds: 0.83\n",
      "Epoch   6/50 Batch  275/485 - Loss:  1.735, Seconds: 1.79\n",
      "Epoch   6/50 Batch  280/485 - Loss:  1.712, Seconds: 0.84\n",
      "Epoch   6/50 Batch  285/485 - Loss:  1.883, Seconds: 0.77\n",
      "Epoch   6/50 Batch  290/485 - Loss:  1.736, Seconds: 0.83\n",
      "Epoch   6/50 Batch  295/485 - Loss:  1.618, Seconds: 0.90\n",
      "Epoch   6/50 Batch  300/485 - Loss:  1.640, Seconds: 0.83\n",
      "Epoch   6/50 Batch  305/485 - Loss:  1.791, Seconds: 0.85\n",
      "Epoch   6/50 Batch  310/485 - Loss:  1.842, Seconds: 0.74\n",
      "Epoch   6/50 Batch  315/485 - Loss:  1.888, Seconds: 0.81\n",
      "Epoch   6/50 Batch  320/485 - Loss:  1.890, Seconds: 0.92\n",
      "Average loss for this update: 1.74\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  325/485 - Loss:  2.025, Seconds: 0.81\n",
      "Epoch   6/50 Batch  330/485 - Loss:  1.668, Seconds: 0.91\n",
      "Epoch   6/50 Batch  335/485 - Loss:  1.939, Seconds: 2.21\n",
      "Epoch   6/50 Batch  340/485 - Loss:  1.910, Seconds: 1.04\n",
      "Epoch   6/50 Batch  345/485 - Loss:  1.936, Seconds: 0.90\n",
      "Epoch   6/50 Batch  350/485 - Loss:  1.997, Seconds: 0.88\n",
      "Epoch   6/50 Batch  355/485 - Loss:  1.841, Seconds: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/50 Batch  360/485 - Loss:  1.848, Seconds: 0.98\n",
      "Epoch   6/50 Batch  365/485 - Loss:  1.987, Seconds: 1.03\n",
      "Epoch   6/50 Batch  370/485 - Loss:  1.990, Seconds: 0.96\n",
      "Epoch   6/50 Batch  375/485 - Loss:  2.025, Seconds: 0.95\n",
      "Epoch   6/50 Batch  380/485 - Loss:  1.946, Seconds: 1.01\n",
      "Epoch   6/50 Batch  385/485 - Loss:  1.958, Seconds: 0.97\n",
      "Epoch   6/50 Batch  390/485 - Loss:  1.833, Seconds: 1.06\n",
      "Epoch   6/50 Batch  395/485 - Loss:  2.050, Seconds: 0.98\n",
      "Epoch   6/50 Batch  400/485 - Loss:  1.991, Seconds: 1.05\n",
      "Epoch   6/50 Batch  405/485 - Loss:  1.947, Seconds: 1.10\n",
      "Epoch   6/50 Batch  410/485 - Loss:  1.929, Seconds: 1.15\n",
      "Epoch   6/50 Batch  415/485 - Loss:  2.079, Seconds: 1.22\n",
      "Epoch   6/50 Batch  420/485 - Loss:  2.124, Seconds: 1.22\n",
      "Epoch   6/50 Batch  425/485 - Loss:  1.967, Seconds: 1.17\n",
      "Epoch   6/50 Batch  430/485 - Loss:  2.077, Seconds: 1.27\n",
      "Epoch   6/50 Batch  435/485 - Loss:  2.162, Seconds: 2.51\n",
      "Epoch   6/50 Batch  440/485 - Loss:  2.174, Seconds: 1.29\n",
      "Epoch   6/50 Batch  445/485 - Loss:  2.150, Seconds: 1.29\n",
      "Epoch   6/50 Batch  450/485 - Loss:  2.155, Seconds: 1.30\n",
      "Epoch   6/50 Batch  455/485 - Loss:  2.065, Seconds: 1.39\n",
      "Epoch   6/50 Batch  460/485 - Loss:  2.359, Seconds: 1.38\n",
      "Epoch   6/50 Batch  465/485 - Loss:  2.252, Seconds: 1.62\n",
      "Epoch   6/50 Batch  470/485 - Loss:  2.331, Seconds: 1.75\n",
      "Epoch   6/50 Batch  475/485 - Loss:  2.301, Seconds: 1.89\n",
      "Epoch   6/50 Batch  480/485 - Loss:  2.385, Seconds: 1.97\n",
      "Average loss for this update: 2.044\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch    5/485 - Loss:  2.039, Seconds: 0.50\n",
      "Epoch   7/50 Batch   10/485 - Loss:  1.511, Seconds: 0.53\n",
      "Epoch   7/50 Batch   15/485 - Loss:  1.515, Seconds: 0.47\n",
      "Epoch   7/50 Batch   20/485 - Loss:  1.442, Seconds: 0.60\n",
      "Epoch   7/50 Batch   25/485 - Loss:  1.582, Seconds: 0.47\n",
      "Epoch   7/50 Batch   30/485 - Loss:  1.455, Seconds: 0.64\n",
      "Epoch   7/50 Batch   35/485 - Loss:  1.594, Seconds: 0.62\n",
      "Epoch   7/50 Batch   40/485 - Loss:  1.622, Seconds: 0.49\n",
      "Epoch   7/50 Batch   45/485 - Loss:  1.606, Seconds: 0.61\n",
      "Epoch   7/50 Batch   50/485 - Loss:  1.566, Seconds: 0.65\n",
      "Epoch   7/50 Batch   55/485 - Loss:  1.561, Seconds: 0.46\n",
      "Epoch   7/50 Batch   60/485 - Loss:  1.534, Seconds: 0.46\n",
      "Epoch   7/50 Batch   65/485 - Loss:  1.512, Seconds: 0.54\n",
      "Epoch   7/50 Batch   70/485 - Loss:  1.422, Seconds: 0.58\n",
      "Epoch   7/50 Batch   75/485 - Loss:  1.638, Seconds: 0.64\n",
      "Epoch   7/50 Batch   80/485 - Loss:  1.482, Seconds: 0.61\n",
      "Epoch   7/50 Batch   85/485 - Loss:  1.501, Seconds: 0.59\n",
      "Epoch   7/50 Batch   90/485 - Loss:  1.524, Seconds: 0.54\n",
      "Epoch   7/50 Batch   95/485 - Loss:  1.485, Seconds: 0.56\n",
      "Epoch   7/50 Batch  100/485 - Loss:  1.418, Seconds: 1.33\n",
      "Epoch   7/50 Batch  105/485 - Loss:  1.483, Seconds: 1.34\n",
      "Epoch   7/50 Batch  110/485 - Loss:  1.526, Seconds: 0.66\n",
      "Epoch   7/50 Batch  115/485 - Loss:  1.626, Seconds: 0.69\n",
      "Epoch   7/50 Batch  120/485 - Loss:  1.498, Seconds: 0.62\n",
      "Epoch   7/50 Batch  125/485 - Loss:  1.618, Seconds: 0.62\n",
      "Epoch   7/50 Batch  130/485 - Loss:  1.507, Seconds: 0.57\n",
      "Epoch   7/50 Batch  135/485 - Loss:  1.529, Seconds: 0.68\n",
      "Epoch   7/50 Batch  140/485 - Loss:  1.713, Seconds: 0.59\n",
      "Epoch   7/50 Batch  145/485 - Loss:  1.538, Seconds: 0.66\n",
      "Epoch   7/50 Batch  150/485 - Loss:  1.608, Seconds: 0.64\n",
      "Epoch   7/50 Batch  155/485 - Loss:  1.552, Seconds: 0.64\n",
      "Epoch   7/50 Batch  160/485 - Loss:  1.472, Seconds: 0.56\n",
      "Average loss for this update: 1.552\n",
      "New Record!\n",
      "Epoch   7/50 Batch  165/485 - Loss:  1.609, Seconds: 0.61\n",
      "Epoch   7/50 Batch  170/485 - Loss:  1.616, Seconds: 0.65\n",
      "Epoch   7/50 Batch  175/485 - Loss:  1.467, Seconds: 0.72\n",
      "Epoch   7/50 Batch  180/485 - Loss:  1.529, Seconds: 0.71\n",
      "Epoch   7/50 Batch  185/485 - Loss:  1.616, Seconds: 0.66\n",
      "Epoch   7/50 Batch  190/485 - Loss:  1.571, Seconds: 0.77\n",
      "Epoch   7/50 Batch  195/485 - Loss:  1.609, Seconds: 0.66\n",
      "Epoch   7/50 Batch  200/485 - Loss:  1.705, Seconds: 0.66\n",
      "Epoch   7/50 Batch  205/485 - Loss:  1.699, Seconds: 0.63\n",
      "Epoch   7/50 Batch  210/485 - Loss:  1.673, Seconds: 0.81\n",
      "Epoch   7/50 Batch  215/485 - Loss:  1.649, Seconds: 0.76\n",
      "Epoch   7/50 Batch  220/485 - Loss:  1.611, Seconds: 0.78\n",
      "Epoch   7/50 Batch  225/485 - Loss:  1.664, Seconds: 0.63\n",
      "Epoch   7/50 Batch  230/485 - Loss:  1.596, Seconds: 0.78\n",
      "Epoch   7/50 Batch  235/485 - Loss:  1.614, Seconds: 0.78\n",
      "Epoch   7/50 Batch  240/485 - Loss:  1.759, Seconds: 0.63\n",
      "Epoch   7/50 Batch  245/485 - Loss:  1.623, Seconds: 0.75\n",
      "Epoch   7/50 Batch  250/485 - Loss:  1.619, Seconds: 0.83\n",
      "Epoch   7/50 Batch  255/485 - Loss:  1.737, Seconds: 0.82\n",
      "Epoch   7/50 Batch  260/485 - Loss:  1.796, Seconds: 0.82\n",
      "Epoch   7/50 Batch  265/485 - Loss:  1.684, Seconds: 0.65\n",
      "Epoch   7/50 Batch  270/485 - Loss:  1.779, Seconds: 0.84\n",
      "Epoch   7/50 Batch  275/485 - Loss:  1.639, Seconds: 0.82\n",
      "Epoch   7/50 Batch  280/485 - Loss:  1.659, Seconds: 0.82\n",
      "Epoch   7/50 Batch  285/485 - Loss:  1.832, Seconds: 0.76\n",
      "Epoch   7/50 Batch  290/485 - Loss:  1.662, Seconds: 0.81\n",
      "Epoch   7/50 Batch  295/485 - Loss:  1.553, Seconds: 0.87\n",
      "Epoch   7/50 Batch  300/485 - Loss:  1.589, Seconds: 0.83\n",
      "Epoch   7/50 Batch  305/485 - Loss:  1.722, Seconds: 0.87\n",
      "Epoch   7/50 Batch  310/485 - Loss:  1.741, Seconds: 0.77\n",
      "Epoch   7/50 Batch  315/485 - Loss:  1.787, Seconds: 0.83\n",
      "Epoch   7/50 Batch  320/485 - Loss:  1.820, Seconds: 0.88\n",
      "Average loss for this update: 1.663\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  325/485 - Loss:  1.929, Seconds: 0.77\n",
      "Epoch   7/50 Batch  330/485 - Loss:  1.607, Seconds: 0.94\n",
      "Epoch   7/50 Batch  335/485 - Loss:  1.828, Seconds: 0.87\n",
      "Epoch   7/50 Batch  340/485 - Loss:  1.831, Seconds: 0.88\n",
      "Epoch   7/50 Batch  345/485 - Loss:  1.888, Seconds: 0.88\n",
      "Epoch   7/50 Batch  350/485 - Loss:  1.944, Seconds: 0.89\n",
      "Epoch   7/50 Batch  355/485 - Loss:  1.769, Seconds: 0.93\n",
      "Epoch   7/50 Batch  360/485 - Loss:  1.759, Seconds: 1.02\n",
      "Epoch   7/50 Batch  365/485 - Loss:  1.885, Seconds: 0.99\n",
      "Epoch   7/50 Batch  370/485 - Loss:  1.910, Seconds: 0.98\n",
      "Epoch   7/50 Batch  375/485 - Loss:  1.932, Seconds: 0.94\n",
      "Epoch   7/50 Batch  380/485 - Loss:  1.903, Seconds: 1.02\n",
      "Epoch   7/50 Batch  385/485 - Loss:  1.894, Seconds: 0.96\n",
      "Epoch   7/50 Batch  390/485 - Loss:  1.742, Seconds: 1.03\n",
      "Epoch   7/50 Batch  395/485 - Loss:  1.983, Seconds: 1.04\n",
      "Epoch   7/50 Batch  400/485 - Loss:  1.898, Seconds: 1.09\n",
      "Epoch   7/50 Batch  405/485 - Loss:  1.878, Seconds: 1.11\n",
      "Epoch   7/50 Batch  410/485 - Loss:  1.856, Seconds: 1.16\n",
      "Epoch   7/50 Batch  415/485 - Loss:  2.003, Seconds: 1.27\n",
      "Epoch   7/50 Batch  420/485 - Loss:  2.021, Seconds: 1.21\n",
      "Epoch   7/50 Batch  425/485 - Loss:  1.891, Seconds: 1.13\n",
      "Epoch   7/50 Batch  430/485 - Loss:  2.015, Seconds: 1.25\n",
      "Epoch   7/50 Batch  435/485 - Loss:  2.076, Seconds: 1.24\n",
      "Epoch   7/50 Batch  440/485 - Loss:  2.132, Seconds: 1.28\n",
      "Epoch   7/50 Batch  445/485 - Loss:  2.075, Seconds: 1.29\n",
      "Epoch   7/50 Batch  450/485 - Loss:  2.088, Seconds: 1.32\n",
      "Epoch   7/50 Batch  455/485 - Loss:  1.979, Seconds: 1.42\n",
      "Epoch   7/50 Batch  460/485 - Loss:  2.285, Seconds: 1.39\n",
      "Epoch   7/50 Batch  465/485 - Loss:  2.188, Seconds: 1.60\n",
      "Epoch   7/50 Batch  470/485 - Loss:  2.264, Seconds: 1.69\n",
      "Epoch   7/50 Batch  475/485 - Loss:  2.223, Seconds: 1.86\n",
      "Epoch   7/50 Batch  480/485 - Loss:  2.315, Seconds: 2.13\n",
      "Average loss for this update: 1.968\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch    5/485 - Loss:  2.102, Seconds: 0.53\n",
      "Epoch   8/50 Batch   10/485 - Loss:  1.430, Seconds: 0.59\n",
      "Epoch   8/50 Batch   15/485 - Loss:  1.485, Seconds: 0.49\n",
      "Epoch   8/50 Batch   20/485 - Loss:  1.370, Seconds: 0.60\n",
      "Epoch   8/50 Batch   25/485 - Loss:  1.493, Seconds: 0.43\n",
      "Epoch   8/50 Batch   30/485 - Loss:  1.363, Seconds: 0.63\n",
      "Epoch   8/50 Batch   35/485 - Loss:  1.512, Seconds: 0.57\n",
      "Epoch   8/50 Batch   40/485 - Loss:  1.551, Seconds: 0.47\n",
      "Epoch   8/50 Batch   45/485 - Loss:  1.509, Seconds: 0.59\n",
      "Epoch   8/50 Batch   50/485 - Loss:  1.487, Seconds: 0.64\n",
      "Epoch   8/50 Batch   55/485 - Loss:  1.470, Seconds: 0.48\n",
      "Epoch   8/50 Batch   60/485 - Loss:  1.441, Seconds: 0.48\n",
      "Epoch   8/50 Batch   65/485 - Loss:  1.416, Seconds: 0.52\n",
      "Epoch   8/50 Batch   70/485 - Loss:  1.344, Seconds: 0.61\n",
      "Epoch   8/50 Batch   75/485 - Loss:  1.545, Seconds: 0.64\n",
      "Epoch   8/50 Batch   80/485 - Loss:  1.424, Seconds: 0.61\n",
      "Epoch   8/50 Batch   85/485 - Loss:  1.450, Seconds: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/50 Batch   90/485 - Loss:  1.449, Seconds: 0.53\n",
      "Epoch   8/50 Batch   95/485 - Loss:  1.435, Seconds: 0.56\n",
      "Epoch   8/50 Batch  100/485 - Loss:  1.328, Seconds: 0.74\n",
      "Epoch   8/50 Batch  105/485 - Loss:  1.405, Seconds: 0.77\n",
      "Epoch   8/50 Batch  110/485 - Loss:  1.425, Seconds: 0.65\n",
      "Epoch   8/50 Batch  115/485 - Loss:  1.549, Seconds: 0.68\n",
      "Epoch   8/50 Batch  120/485 - Loss:  1.421, Seconds: 0.64\n",
      "Epoch   8/50 Batch  125/485 - Loss:  1.483, Seconds: 0.59\n",
      "Epoch   8/50 Batch  130/485 - Loss:  1.423, Seconds: 0.60\n",
      "Epoch   8/50 Batch  135/485 - Loss:  1.481, Seconds: 0.71\n",
      "Epoch   8/50 Batch  140/485 - Loss:  1.587, Seconds: 0.66\n",
      "Epoch   8/50 Batch  145/485 - Loss:  1.469, Seconds: 0.66\n",
      "Epoch   8/50 Batch  150/485 - Loss:  1.533, Seconds: 0.69\n",
      "Epoch   8/50 Batch  155/485 - Loss:  1.497, Seconds: 0.74\n",
      "Epoch   8/50 Batch  160/485 - Loss:  1.412, Seconds: 0.59\n",
      "Average loss for this update: 1.478\n",
      "New Record!\n",
      "Epoch   8/50 Batch  165/485 - Loss:  1.510, Seconds: 0.62\n",
      "Epoch   8/50 Batch  170/485 - Loss:  1.524, Seconds: 0.66\n",
      "Epoch   8/50 Batch  175/485 - Loss:  1.400, Seconds: 0.70\n",
      "Epoch   8/50 Batch  180/485 - Loss:  1.464, Seconds: 0.70\n",
      "Epoch   8/50 Batch  185/485 - Loss:  1.554, Seconds: 0.65\n",
      "Epoch   8/50 Batch  190/485 - Loss:  1.501, Seconds: 0.78\n",
      "Epoch   8/50 Batch  195/485 - Loss:  1.530, Seconds: 0.68\n",
      "Epoch   8/50 Batch  200/485 - Loss:  1.622, Seconds: 0.67\n",
      "Epoch   8/50 Batch  205/485 - Loss:  1.612, Seconds: 0.63\n",
      "Epoch   8/50 Batch  210/485 - Loss:  1.616, Seconds: 0.82\n",
      "Epoch   8/50 Batch  215/485 - Loss:  1.554, Seconds: 0.74\n",
      "Epoch   8/50 Batch  220/485 - Loss:  1.551, Seconds: 0.74\n",
      "Epoch   8/50 Batch  225/485 - Loss:  1.600, Seconds: 0.62\n",
      "Epoch   8/50 Batch  230/485 - Loss:  1.555, Seconds: 0.77\n",
      "Epoch   8/50 Batch  235/485 - Loss:  1.565, Seconds: 0.78\n",
      "Epoch   8/50 Batch  240/485 - Loss:  1.695, Seconds: 0.62\n",
      "Epoch   8/50 Batch  245/485 - Loss:  1.545, Seconds: 0.75\n",
      "Epoch   8/50 Batch  250/485 - Loss:  1.538, Seconds: 0.81\n",
      "Epoch   8/50 Batch  255/485 - Loss:  1.654, Seconds: 0.84\n",
      "Epoch   8/50 Batch  260/485 - Loss:  1.676, Seconds: 0.81\n",
      "Epoch   8/50 Batch  265/485 - Loss:  1.614, Seconds: 0.75\n",
      "Epoch   8/50 Batch  270/485 - Loss:  1.672, Seconds: 0.80\n",
      "Epoch   8/50 Batch  275/485 - Loss:  1.553, Seconds: 0.89\n",
      "Epoch   8/50 Batch  280/485 - Loss:  1.573, Seconds: 0.85\n",
      "Epoch   8/50 Batch  285/485 - Loss:  1.752, Seconds: 0.83\n",
      "Epoch   8/50 Batch  290/485 - Loss:  1.607, Seconds: 0.89\n",
      "Epoch   8/50 Batch  295/485 - Loss:  1.495, Seconds: 0.86\n",
      "Epoch   8/50 Batch  300/485 - Loss:  1.516, Seconds: 0.82\n",
      "Epoch   8/50 Batch  305/485 - Loss:  1.641, Seconds: 0.83\n",
      "Epoch   8/50 Batch  310/485 - Loss:  1.664, Seconds: 0.76\n",
      "Epoch   8/50 Batch  315/485 - Loss:  1.695, Seconds: 0.83\n",
      "Epoch   8/50 Batch  320/485 - Loss:  1.738, Seconds: 0.89\n",
      "Average loss for this update: 1.587\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  325/485 - Loss:  1.864, Seconds: 0.77\n",
      "Epoch   8/50 Batch  330/485 - Loss:  1.529, Seconds: 0.92\n",
      "Epoch   8/50 Batch  335/485 - Loss:  1.743, Seconds: 0.90\n",
      "Epoch   8/50 Batch  340/485 - Loss:  1.739, Seconds: 0.87\n",
      "Epoch   8/50 Batch  345/485 - Loss:  1.794, Seconds: 0.86\n",
      "Epoch   8/50 Batch  350/485 - Loss:  1.865, Seconds: 0.85\n",
      "Epoch   8/50 Batch  355/485 - Loss:  1.709, Seconds: 0.95\n",
      "Epoch   8/50 Batch  360/485 - Loss:  1.685, Seconds: 0.98\n",
      "Epoch   8/50 Batch  365/485 - Loss:  1.792, Seconds: 0.99\n",
      "Epoch   8/50 Batch  370/485 - Loss:  1.797, Seconds: 0.97\n",
      "Epoch   8/50 Batch  375/485 - Loss:  1.870, Seconds: 0.94\n",
      "Epoch   8/50 Batch  380/485 - Loss:  1.815, Seconds: 1.02\n",
      "Epoch   8/50 Batch  385/485 - Loss:  1.810, Seconds: 1.00\n",
      "Epoch   8/50 Batch  390/485 - Loss:  1.647, Seconds: 1.05\n",
      "Epoch   8/50 Batch  395/485 - Loss:  1.886, Seconds: 1.02\n",
      "Epoch   8/50 Batch  400/485 - Loss:  1.828, Seconds: 1.07\n",
      "Epoch   8/50 Batch  405/485 - Loss:  1.811, Seconds: 1.09\n",
      "Epoch   8/50 Batch  410/485 - Loss:  1.791, Seconds: 1.13\n",
      "Epoch   8/50 Batch  415/485 - Loss:  1.906, Seconds: 1.18\n",
      "Epoch   8/50 Batch  420/485 - Loss:  1.930, Seconds: 1.15\n",
      "Epoch   8/50 Batch  425/485 - Loss:  1.794, Seconds: 1.15\n",
      "Epoch   8/50 Batch  430/485 - Loss:  1.925, Seconds: 1.18\n",
      "Epoch   8/50 Batch  435/485 - Loss:  1.991, Seconds: 1.22\n",
      "Epoch   8/50 Batch  440/485 - Loss:  1.994, Seconds: 1.27\n",
      "Epoch   8/50 Batch  445/485 - Loss:  1.990, Seconds: 1.29\n",
      "Epoch   8/50 Batch  450/485 - Loss:  2.005, Seconds: 1.28\n",
      "Epoch   8/50 Batch  455/485 - Loss:  1.907, Seconds: 1.38\n",
      "Epoch   8/50 Batch  460/485 - Loss:  2.190, Seconds: 1.42\n",
      "Epoch   8/50 Batch  465/485 - Loss:  2.103, Seconds: 1.70\n",
      "Epoch   8/50 Batch  470/485 - Loss:  2.161, Seconds: 1.64\n",
      "Epoch   8/50 Batch  475/485 - Loss:  2.137, Seconds: 1.82\n",
      "Epoch   8/50 Batch  480/485 - Loss:  2.244, Seconds: 1.88\n",
      "Average loss for this update: 1.883\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch    5/485 - Loss:  1.906, Seconds: 0.50\n",
      "Epoch   9/50 Batch   10/485 - Loss:  1.424, Seconds: 0.54\n",
      "Epoch   9/50 Batch   15/485 - Loss:  1.411, Seconds: 0.46\n",
      "Epoch   9/50 Batch   20/485 - Loss:  1.285, Seconds: 0.61\n",
      "Epoch   9/50 Batch   25/485 - Loss:  1.400, Seconds: 0.45\n",
      "Epoch   9/50 Batch   30/485 - Loss:  1.334, Seconds: 0.61\n",
      "Epoch   9/50 Batch   35/485 - Loss:  1.440, Seconds: 0.56\n",
      "Epoch   9/50 Batch   40/485 - Loss:  1.444, Seconds: 0.46\n",
      "Epoch   9/50 Batch   45/485 - Loss:  1.428, Seconds: 0.56\n",
      "Epoch   9/50 Batch   50/485 - Loss:  1.414, Seconds: 0.62\n",
      "Epoch   9/50 Batch   55/485 - Loss:  1.404, Seconds: 0.42\n",
      "Epoch   9/50 Batch   60/485 - Loss:  1.338, Seconds: 0.44\n",
      "Epoch   9/50 Batch   65/485 - Loss:  1.341, Seconds: 0.50\n",
      "Epoch   9/50 Batch   70/485 - Loss:  1.293, Seconds: 0.55\n",
      "Epoch   9/50 Batch   75/485 - Loss:  1.459, Seconds: 0.60\n",
      "Epoch   9/50 Batch   80/485 - Loss:  1.330, Seconds: 0.59\n",
      "Epoch   9/50 Batch   85/485 - Loss:  1.345, Seconds: 0.56\n",
      "Epoch   9/50 Batch   90/485 - Loss:  1.376, Seconds: 0.52\n",
      "Epoch   9/50 Batch   95/485 - Loss:  1.362, Seconds: 0.56\n",
      "Epoch   9/50 Batch  100/485 - Loss:  1.268, Seconds: 0.70\n",
      "Epoch   9/50 Batch  105/485 - Loss:  1.379, Seconds: 0.70\n",
      "Epoch   9/50 Batch  110/485 - Loss:  1.362, Seconds: 0.61\n",
      "Epoch   9/50 Batch  115/485 - Loss:  1.496, Seconds: 0.66\n",
      "Epoch   9/50 Batch  120/485 - Loss:  1.340, Seconds: 0.62\n",
      "Epoch   9/50 Batch  125/485 - Loss:  1.445, Seconds: 0.59\n",
      "Epoch   9/50 Batch  130/485 - Loss:  1.361, Seconds: 0.55\n",
      "Epoch   9/50 Batch  135/485 - Loss:  1.418, Seconds: 0.68\n",
      "Epoch   9/50 Batch  140/485 - Loss:  1.505, Seconds: 0.60\n",
      "Epoch   9/50 Batch  145/485 - Loss:  1.390, Seconds: 0.65\n",
      "Epoch   9/50 Batch  150/485 - Loss:  1.434, Seconds: 0.61\n",
      "Epoch   9/50 Batch  155/485 - Loss:  1.421, Seconds: 0.66\n",
      "Epoch   9/50 Batch  160/485 - Loss:  1.359, Seconds: 0.58\n",
      "Average loss for this update: 1.404\n",
      "New Record!\n",
      "Epoch   9/50 Batch  165/485 - Loss:  1.466, Seconds: 0.61\n",
      "Epoch   9/50 Batch  170/485 - Loss:  1.477, Seconds: 0.65\n",
      "Epoch   9/50 Batch  175/485 - Loss:  1.339, Seconds: 0.70\n",
      "Epoch   9/50 Batch  180/485 - Loss:  1.397, Seconds: 0.70\n",
      "Epoch   9/50 Batch  185/485 - Loss:  1.481, Seconds: 0.64\n",
      "Epoch   9/50 Batch  190/485 - Loss:  1.436, Seconds: 0.76\n",
      "Epoch   9/50 Batch  195/485 - Loss:  1.465, Seconds: 0.64\n",
      "Epoch   9/50 Batch  200/485 - Loss:  1.577, Seconds: 0.63\n",
      "Epoch   9/50 Batch  205/485 - Loss:  1.515, Seconds: 0.59\n",
      "Epoch   9/50 Batch  210/485 - Loss:  1.534, Seconds: 0.76\n",
      "Epoch   9/50 Batch  215/485 - Loss:  1.460, Seconds: 0.70\n",
      "Epoch   9/50 Batch  220/485 - Loss:  1.491, Seconds: 0.74\n",
      "Epoch   9/50 Batch  225/485 - Loss:  1.541, Seconds: 0.61\n",
      "Epoch   9/50 Batch  230/485 - Loss:  1.494, Seconds: 0.75\n",
      "Epoch   9/50 Batch  235/485 - Loss:  1.475, Seconds: 0.75\n",
      "Epoch   9/50 Batch  240/485 - Loss:  1.605, Seconds: 0.60\n",
      "Epoch   9/50 Batch  245/485 - Loss:  1.489, Seconds: 0.72\n",
      "Epoch   9/50 Batch  250/485 - Loss:  1.478, Seconds: 0.81\n",
      "Epoch   9/50 Batch  255/485 - Loss:  1.602, Seconds: 0.81\n",
      "Epoch   9/50 Batch  260/485 - Loss:  1.595, Seconds: 0.82\n",
      "Epoch   9/50 Batch  265/485 - Loss:  1.544, Seconds: 0.68\n",
      "Epoch   9/50 Batch  270/485 - Loss:  1.610, Seconds: 0.77\n",
      "Epoch   9/50 Batch  275/485 - Loss:  1.524, Seconds: 0.84\n",
      "Epoch   9/50 Batch  280/485 - Loss:  1.510, Seconds: 0.80\n",
      "Epoch   9/50 Batch  285/485 - Loss:  1.662, Seconds: 0.76\n",
      "Epoch   9/50 Batch  290/485 - Loss:  1.523, Seconds: 0.80\n",
      "Epoch   9/50 Batch  295/485 - Loss:  1.437, Seconds: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/50 Batch  300/485 - Loss:  1.459, Seconds: 0.85\n",
      "Epoch   9/50 Batch  305/485 - Loss:  1.600, Seconds: 0.83\n",
      "Epoch   9/50 Batch  310/485 - Loss:  1.624, Seconds: 0.74\n",
      "Epoch   9/50 Batch  315/485 - Loss:  1.623, Seconds: 0.82\n",
      "Epoch   9/50 Batch  320/485 - Loss:  1.655, Seconds: 0.88\n",
      "Average loss for this update: 1.522\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  325/485 - Loss:  1.752, Seconds: 0.74\n",
      "Epoch   9/50 Batch  330/485 - Loss:  1.468, Seconds: 0.94\n",
      "Epoch   9/50 Batch  335/485 - Loss:  1.692, Seconds: 0.90\n",
      "Epoch   9/50 Batch  340/485 - Loss:  1.677, Seconds: 0.85\n",
      "Epoch   9/50 Batch  345/485 - Loss:  1.691, Seconds: 0.86\n",
      "Epoch   9/50 Batch  350/485 - Loss:  1.781, Seconds: 0.87\n",
      "Epoch   9/50 Batch  355/485 - Loss:  1.619, Seconds: 0.94\n",
      "Epoch   9/50 Batch  360/485 - Loss:  1.636, Seconds: 0.99\n",
      "Epoch   9/50 Batch  365/485 - Loss:  1.747, Seconds: 0.99\n",
      "Epoch   9/50 Batch  370/485 - Loss:  1.725, Seconds: 0.96\n",
      "Epoch   9/50 Batch  375/485 - Loss:  1.780, Seconds: 1.01\n",
      "Epoch   9/50 Batch  380/485 - Loss:  1.758, Seconds: 1.06\n",
      "Epoch   9/50 Batch  385/485 - Loss:  1.747, Seconds: 0.97\n",
      "Epoch   9/50 Batch  390/485 - Loss:  1.597, Seconds: 1.15\n",
      "Epoch   9/50 Batch  395/485 - Loss:  1.814, Seconds: 1.08\n",
      "Epoch   9/50 Batch  400/485 - Loss:  1.756, Seconds: 1.05\n",
      "Epoch   9/50 Batch  405/485 - Loss:  1.744, Seconds: 1.12\n",
      "Epoch   9/50 Batch  410/485 - Loss:  1.735, Seconds: 1.13\n",
      "Epoch   9/50 Batch  415/485 - Loss:  1.849, Seconds: 1.19\n",
      "Epoch   9/50 Batch  420/485 - Loss:  1.877, Seconds: 1.15\n",
      "Epoch   9/50 Batch  425/485 - Loss:  1.743, Seconds: 1.19\n",
      "Epoch   9/50 Batch  430/485 - Loss:  1.872, Seconds: 1.32\n",
      "Epoch   9/50 Batch  435/485 - Loss:  1.912, Seconds: 1.25\n",
      "Epoch   9/50 Batch  440/485 - Loss:  1.926, Seconds: 1.29\n",
      "Epoch   9/50 Batch  445/485 - Loss:  1.952, Seconds: 1.32\n",
      "Epoch   9/50 Batch  450/485 - Loss:  1.933, Seconds: 1.29\n",
      "Epoch   9/50 Batch  455/485 - Loss:  1.855, Seconds: 1.39\n",
      "Epoch   9/50 Batch  460/485 - Loss:  2.124, Seconds: 1.37\n",
      "Epoch   9/50 Batch  465/485 - Loss:  2.049, Seconds: 1.58\n",
      "Epoch   9/50 Batch  470/485 - Loss:  2.115, Seconds: 1.70\n",
      "Epoch   9/50 Batch  475/485 - Loss:  2.065, Seconds: 1.90\n",
      "Epoch   9/50 Batch  480/485 - Loss:  2.186, Seconds: 1.86\n",
      "Average loss for this update: 1.818\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch    5/485 - Loss:  1.845, Seconds: 0.51\n",
      "Epoch  10/50 Batch   10/485 - Loss:  1.372, Seconds: 0.54\n",
      "Epoch  10/50 Batch   15/485 - Loss:  1.396, Seconds: 0.47\n",
      "Epoch  10/50 Batch   20/485 - Loss:  1.249, Seconds: 0.59\n",
      "Epoch  10/50 Batch   25/485 - Loss:  1.350, Seconds: 0.43\n",
      "Epoch  10/50 Batch   30/485 - Loss:  1.298, Seconds: 0.60\n",
      "Epoch  10/50 Batch   35/485 - Loss:  1.386, Seconds: 0.56\n",
      "Epoch  10/50 Batch   40/485 - Loss:  1.401, Seconds: 0.45\n",
      "Epoch  10/50 Batch   45/485 - Loss:  1.377, Seconds: 0.57\n",
      "Epoch  10/50 Batch   50/485 - Loss:  1.352, Seconds: 0.63\n",
      "Epoch  10/50 Batch   55/485 - Loss:  1.366, Seconds: 0.43\n",
      "Epoch  10/50 Batch   60/485 - Loss:  1.301, Seconds: 0.43\n",
      "Epoch  10/50 Batch   65/485 - Loss:  1.291, Seconds: 0.51\n",
      "Epoch  10/50 Batch   70/485 - Loss:  1.222, Seconds: 0.55\n",
      "Epoch  10/50 Batch   75/485 - Loss:  1.414, Seconds: 0.59\n",
      "Epoch  10/50 Batch   80/485 - Loss:  1.294, Seconds: 0.60\n",
      "Epoch  10/50 Batch   85/485 - Loss:  1.311, Seconds: 0.56\n",
      "Epoch  10/50 Batch   90/485 - Loss:  1.319, Seconds: 0.52\n",
      "Epoch  10/50 Batch   95/485 - Loss:  1.301, Seconds: 0.56\n",
      "Epoch  10/50 Batch  100/485 - Loss:  1.225, Seconds: 0.68\n",
      "Epoch  10/50 Batch  105/485 - Loss:  1.293, Seconds: 0.69\n",
      "Epoch  10/50 Batch  110/485 - Loss:  1.299, Seconds: 0.63\n",
      "Epoch  10/50 Batch  115/485 - Loss:  1.425, Seconds: 0.66\n",
      "Epoch  10/50 Batch  120/485 - Loss:  1.287, Seconds: 0.63\n",
      "Epoch  10/50 Batch  125/485 - Loss:  1.374, Seconds: 0.59\n",
      "Epoch  10/50 Batch  130/485 - Loss:  1.319, Seconds: 0.56\n",
      "Epoch  10/50 Batch  135/485 - Loss:  1.367, Seconds: 0.67\n",
      "Epoch  10/50 Batch  140/485 - Loss:  1.469, Seconds: 0.61\n",
      "Epoch  10/50 Batch  145/485 - Loss:  1.336, Seconds: 0.64\n",
      "Epoch  10/50 Batch  150/485 - Loss:  1.401, Seconds: 0.62\n",
      "Epoch  10/50 Batch  155/485 - Loss:  1.358, Seconds: 0.65\n",
      "Epoch  10/50 Batch  160/485 - Loss:  1.296, Seconds: 0.56\n",
      "Average loss for this update: 1.353\n",
      "New Record!\n",
      "Epoch  10/50 Batch  165/485 - Loss:  1.378, Seconds: 0.61\n",
      "Epoch  10/50 Batch  170/485 - Loss:  1.420, Seconds: 0.65\n",
      "Epoch  10/50 Batch  175/485 - Loss:  1.264, Seconds: 0.69\n",
      "Epoch  10/50 Batch  180/485 - Loss:  1.315, Seconds: 0.69\n",
      "Epoch  10/50 Batch  185/485 - Loss:  1.437, Seconds: 0.61\n",
      "Epoch  10/50 Batch  190/485 - Loss:  1.367, Seconds: 0.75\n",
      "Epoch  10/50 Batch  195/485 - Loss:  1.408, Seconds: 0.62\n",
      "Epoch  10/50 Batch  200/485 - Loss:  1.529, Seconds: 0.62\n",
      "Epoch  10/50 Batch  205/485 - Loss:  1.436, Seconds: 0.61\n",
      "Epoch  10/50 Batch  210/485 - Loss:  1.455, Seconds: 0.76\n",
      "Epoch  10/50 Batch  215/485 - Loss:  1.415, Seconds: 0.77\n",
      "Epoch  10/50 Batch  220/485 - Loss:  1.424, Seconds: 0.71\n",
      "Epoch  10/50 Batch  225/485 - Loss:  1.451, Seconds: 0.62\n",
      "Epoch  10/50 Batch  230/485 - Loss:  1.415, Seconds: 0.73\n",
      "Epoch  10/50 Batch  235/485 - Loss:  1.420, Seconds: 0.80\n",
      "Epoch  10/50 Batch  240/485 - Loss:  1.520, Seconds: 0.60\n",
      "Epoch  10/50 Batch  245/485 - Loss:  1.438, Seconds: 0.71\n",
      "Epoch  10/50 Batch  250/485 - Loss:  1.418, Seconds: 0.83\n",
      "Epoch  10/50 Batch  255/485 - Loss:  1.554, Seconds: 0.82\n",
      "Epoch  10/50 Batch  260/485 - Loss:  1.522, Seconds: 0.77\n",
      "Epoch  10/50 Batch  265/485 - Loss:  1.464, Seconds: 0.65\n",
      "Epoch  10/50 Batch  270/485 - Loss:  1.536, Seconds: 0.72\n",
      "Epoch  10/50 Batch  275/485 - Loss:  1.467, Seconds: 0.78\n",
      "Epoch  10/50 Batch  280/485 - Loss:  1.448, Seconds: 0.80\n",
      "Epoch  10/50 Batch  285/485 - Loss:  1.599, Seconds: 0.83\n",
      "Epoch  10/50 Batch  290/485 - Loss:  1.472, Seconds: 0.86\n",
      "Epoch  10/50 Batch  295/485 - Loss:  1.406, Seconds: 0.88\n",
      "Epoch  10/50 Batch  300/485 - Loss:  1.410, Seconds: 0.87\n",
      "Epoch  10/50 Batch  305/485 - Loss:  1.513, Seconds: 0.87\n",
      "Epoch  10/50 Batch  310/485 - Loss:  1.571, Seconds: 0.79\n",
      "Epoch  10/50 Batch  315/485 - Loss:  1.563, Seconds: 0.86\n",
      "Epoch  10/50 Batch  320/485 - Loss:  1.605, Seconds: 0.90\n",
      "Average loss for this update: 1.457\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch  325/485 - Loss:  1.694, Seconds: 0.80\n",
      "Epoch  10/50 Batch  330/485 - Loss:  1.436, Seconds: 0.96\n",
      "Epoch  10/50 Batch  335/485 - Loss:  1.644, Seconds: 0.90\n",
      "Epoch  10/50 Batch  340/485 - Loss:  1.641, Seconds: 0.87\n",
      "Epoch  10/50 Batch  345/485 - Loss:  1.652, Seconds: 0.88\n",
      "Epoch  10/50 Batch  350/485 - Loss:  1.722, Seconds: 0.85\n",
      "Epoch  10/50 Batch  355/485 - Loss:  1.557, Seconds: 0.93\n",
      "Epoch  10/50 Batch  360/485 - Loss:  1.573, Seconds: 1.02\n",
      "Epoch  10/50 Batch  365/485 - Loss:  1.669, Seconds: 1.04\n",
      "Epoch  10/50 Batch  370/485 - Loss:  1.666, Seconds: 1.00\n",
      "Epoch  10/50 Batch  375/485 - Loss:  1.706, Seconds: 0.95\n",
      "Epoch  10/50 Batch  380/485 - Loss:  1.697, Seconds: 0.98\n",
      "Epoch  10/50 Batch  385/485 - Loss:  1.687, Seconds: 0.95\n",
      "Epoch  10/50 Batch  390/485 - Loss:  1.527, Seconds: 1.02\n",
      "Epoch  10/50 Batch  395/485 - Loss:  1.770, Seconds: 0.98\n",
      "Epoch  10/50 Batch  400/485 - Loss:  1.697, Seconds: 1.09\n",
      "Epoch  10/50 Batch  405/485 - Loss:  1.670, Seconds: 1.13\n",
      "Epoch  10/50 Batch  410/485 - Loss:  1.684, Seconds: 1.11\n",
      "Epoch  10/50 Batch  415/485 - Loss:  1.787, Seconds: 1.26\n",
      "Epoch  10/50 Batch  420/485 - Loss:  1.815, Seconds: 1.31\n",
      "Epoch  10/50 Batch  425/485 - Loss:  1.682, Seconds: 1.18\n",
      "Epoch  10/50 Batch  430/485 - Loss:  1.790, Seconds: 1.20\n",
      "Epoch  10/50 Batch  435/485 - Loss:  1.845, Seconds: 1.23\n",
      "Epoch  10/50 Batch  440/485 - Loss:  1.858, Seconds: 1.42\n",
      "Epoch  10/50 Batch  445/485 - Loss:  1.842, Seconds: 1.42\n",
      "Epoch  10/50 Batch  450/485 - Loss:  1.872, Seconds: 1.28\n",
      "Epoch  10/50 Batch  455/485 - Loss:  1.782, Seconds: 1.38\n",
      "Epoch  10/50 Batch  460/485 - Loss:  2.055, Seconds: 1.47\n",
      "Epoch  10/50 Batch  465/485 - Loss:  1.976, Seconds: 1.59\n",
      "Epoch  10/50 Batch  470/485 - Loss:  2.066, Seconds: 1.68\n",
      "Epoch  10/50 Batch  475/485 - Loss:  2.031, Seconds: 1.85\n",
      "Epoch  10/50 Batch  480/485 - Loss:  2.106, Seconds: 1.90\n",
      "Average loss for this update: 1.756\n",
      "No Improvement.\n",
      "Epoch  11/50 Batch    5/485 - Loss:  2.008, Seconds: 0.49\n",
      "Epoch  11/50 Batch   10/485 - Loss:  1.676, Seconds: 0.54\n",
      "Epoch  11/50 Batch   15/485 - Loss:  1.658, Seconds: 0.46\n",
      "Epoch  11/50 Batch   20/485 - Loss:  1.467, Seconds: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/50 Batch   25/485 - Loss:  1.619, Seconds: 0.42\n",
      "Epoch  11/50 Batch   30/485 - Loss:  1.534, Seconds: 0.59\n",
      "Epoch  11/50 Batch   35/485 - Loss:  1.611, Seconds: 0.57\n",
      "Epoch  11/50 Batch   40/485 - Loss:  1.606, Seconds: 0.44\n",
      "Epoch  11/50 Batch   45/485 - Loss:  1.554, Seconds: 0.57\n",
      "Epoch  11/50 Batch   50/485 - Loss:  1.571, Seconds: 0.63\n",
      "Epoch  11/50 Batch   55/485 - Loss:  1.499, Seconds: 0.42\n",
      "Epoch  11/50 Batch   60/485 - Loss:  1.498, Seconds: 0.43\n",
      "Epoch  11/50 Batch   65/485 - Loss:  1.423, Seconds: 0.50\n",
      "Epoch  11/50 Batch   70/485 - Loss:  1.359, Seconds: 0.53\n",
      "Epoch  11/50 Batch   75/485 - Loss:  1.558, Seconds: 0.59\n",
      "Epoch  11/50 Batch   80/485 - Loss:  1.426, Seconds: 0.57\n",
      "Epoch  11/50 Batch   85/485 - Loss:  1.441, Seconds: 0.56\n",
      "Epoch  11/50 Batch   90/485 - Loss:  1.484, Seconds: 0.53\n",
      "Epoch  11/50 Batch   95/485 - Loss:  1.447, Seconds: 0.56\n",
      "Epoch  11/50 Batch  100/485 - Loss:  1.341, Seconds: 0.69\n",
      "Epoch  11/50 Batch  105/485 - Loss:  1.436, Seconds: 0.69\n",
      "Epoch  11/50 Batch  110/485 - Loss:  1.434, Seconds: 0.62\n",
      "Epoch  11/50 Batch  115/485 - Loss:  1.512, Seconds: 0.65\n",
      "Epoch  11/50 Batch  120/485 - Loss:  1.379, Seconds: 0.61\n",
      "Epoch  11/50 Batch  125/485 - Loss:  1.514, Seconds: 0.58\n",
      "Epoch  11/50 Batch  130/485 - Loss:  1.418, Seconds: 0.54\n",
      "Epoch  11/50 Batch  135/485 - Loss:  1.447, Seconds: 0.67\n",
      "Epoch  11/50 Batch  140/485 - Loss:  1.565, Seconds: 0.59\n",
      "Epoch  11/50 Batch  145/485 - Loss:  1.419, Seconds: 0.66\n",
      "Epoch  11/50 Batch  150/485 - Loss:  1.464, Seconds: 0.59\n",
      "Epoch  11/50 Batch  155/485 - Loss:  1.451, Seconds: 0.63\n",
      "Epoch  11/50 Batch  160/485 - Loss:  1.374, Seconds: 0.57\n",
      "Average loss for this update: 1.506\n",
      "No Improvement.\n",
      "Stopping Training.\n",
      "Model Trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Training will Strat now.\")\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "\n",
    "    #loader = tf.train.import_meta_graph(\"C:/Users/shrey/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/best_model.ckpt.meta\")\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    #by commenting above 2 lines the code will start retrain the model. \n",
    "            \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "        saver = tf.train.Saver() \n",
    "        saver.save(sess, checkpoint)\n",
    "\n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "\n",
    "print(\"Model Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(\"Summary updated loss:\", summary_update_loss[:])\n",
    "#print(\"Updated loss: \", update_loss)\n",
    "\n",
    "plt.plot(summary_update_loss[:])\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text, remove_stopwords=True)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=35\n",
    "sum_F=12.346860213552002\n",
    "avg_F=0.3527674346729143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "count=count-1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text [1511, 94, 174, 485, 2275, 8183, 11707, 14912, 22129, 18367, 2094, 63, 485, 9079, 4103, 1168, 2837, 158, 1439, 8649, 867, 8625, 2500, 21915, 22130, 9492, 7894, 7953, 174, 59, 18, 7481, 0, 613, 5668, 68, 6108, 17678, 14237, 1961, 12776, 174, 745, 9690, 2295, 68, 14346, 2455, 17678, 10949, 14184, 29, 174, 1, 444, 1086, 3016, 174, 476, 174, 1309, 10132, 10219, 3260, 4778, 648, 8929, 3016, 3857, 18496, 1609, 68, 3560, 1176, 1248, 147, 1, 1773, 4522, 8054, 1905, 11364, 438, 174, 722, 686, 370, 174, 3016, 4527, 1328, 3016, 1854, 3450, 2320, 379, 613, 117, 609, 1176, 4466, 22131, 115, 2152, 4304, 8252, 815, 1466, 9267, 1954, 174, 5902, 806, 1786, 21, 372, 1609]\n",
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Index Value:  18978\n",
      "\n",
      "Original Text: If you try to buy tea in one of our major cities and are approached by some limp-wristed purveyor trying to sell you one of his \"artisanal\" blends packaged in a paper bag with an attractive or colorful label, do not scoff but leave the premises in as diplomatic, and polite manner you can muster.  Tea has been and always will be an expensive commodity, and with good reason; its irresistible flavor depends on volatile compounds either inherent in the tea or added to enhance its finished flavor. The operative word here is \"volatile\" which is synonymous with \"fleeting\" and in the world of tea quality, packaging is everything. With Twinings tea you are getting a tea with the maximum protection; a foil-sealed metal can, for goodness sake.  Twinings delivers it to you with the understanding that you will enjoy the flavor that they meant for you to have so that you may return to them as a source of quality teas again and again.  Of course, once you've opened the can it's up to you to keep the can closed after use until the tea is all gone. I prefer loose tea and Twinings is my supplier of choice.  Twinings has so many varieties for you to choose from, there really is no reason to go elsewhere. We may have sent those Redcoats back to England with their tails between their legs but we still have a lot to learn from them when it comes to tea. Lady Grey is an especially tasty blend, you'll enjoy it.\n",
      "File Written to text.001.txt\n",
      "\n",
      "Orignal Summary:  A Tea for Beautiful Woman and Their Equally Handsome Men\n",
      "\n",
      "Text\n",
      "  Word Ids:    [1511, 94, 174, 485, 2275, 8183, 11707, 14912, 22129, 18367, 2094, 63, 485, 9079, 4103, 1168, 2837, 158, 1439, 8649, 867, 8625, 2500, 21915, 22130, 9492, 7894, 7953, 174, 59, 18, 7481, 0, 613, 5668, 68, 6108, 17678, 14237, 1961, 12776, 174, 745, 9690, 2295, 68, 14346, 2455, 17678, 10949, 14184, 29, 174, 1, 444, 1086, 3016, 174, 476, 174, 1309, 10132, 10219, 3260, 4778, 648, 8929, 3016, 3857, 18496, 1609, 68, 3560, 1176, 1248, 147, 1, 1773, 4522, 8054, 1905, 11364, 438, 174, 722, 686, 370, 174, 3016, 4527, 1328, 3016, 1854, 3450, 2320, 379, 613, 117, 609, 1176, 4466, 22131, 115, 2152, 4304, 8252, 815, 1466, 9267, 1954, 174, 5902, 806, 1786, 21, 372, 1609]\n",
      "  Input Words: try buy tea one major cities approached limp wristed purveyor trying sell one artisanal blends packaged paper bag attractive colorful label scoff leave premises diplomatic polite manner muster tea always expensive commodity good reason irresistible flavor depends volatile compounds either inherent tea added enhance finished flavor operative word volatile synonymous fleeting world tea quality packaging everything twinings tea getting tea maximum protection foil sealed metal goodness sake twinings delivers understanding enjoy flavor meant may return source quality teas course opened keep closed use tea gone prefer loose tea twinings supplier choice twinings many varieties choose really reason go elsewhere may sent redcoats back england tails legs still lot learn comes tea lady grey especially tasty blend enjoy\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [13, 174]\n",
      "File Written to text.A.001.txt\n",
      "  Response Words:  great tea\n",
      "Time:  14.701942443847656\n"
     ]
    }
   ],
   "source": [
    "count=count+1\n",
    "\n",
    "\n",
    "##Create your own review or use one from the dataset\n",
    "#input_sentence = \"The packaging and everything was intact, but the yeast I got expired on 12/16.. Not sure if this is still good to use?\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "\n",
    "##Comment the following 3 lines if using custom Summary and text\n",
    "random = np.random.randint(0,25389) #len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "print(\"Cleaned Text\", text)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    ## Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "##    Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "## Remove the padding from the summary\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "elapsed_time=time.time()-start_time\n",
    "\n",
    "##for custom summary and text, comment this line.\n",
    "print(\"Index Value: \",random)\n",
    "\n",
    "##for custom summary and text use \"input_sequence\" as parameter insted of \"reviews.Text.loc[random]\"\n",
    "print('\\nOriginal Text:',reviews.Text.loc[random]) #input_sentence) #\n",
    "\n",
    "##for custom orignal summary and text uncomment the next line and comment the next to next line.\n",
    "#ogSum=\"Expired yeast!\"\n",
    "ogSum=reviews.Summary.loc[random]\n",
    "ogFile=open(\"Testing/text.001.txt\",'w')\n",
    "ogFile.write(ogSum)\n",
    "print(\"File Written to text.001.txt\")\n",
    "ogFile.close()\n",
    "\n",
    "print('\\nOrignal Summary: ',ogSum)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "\n",
    "predSum=\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
    "predFile=open(\"Testing/text.A.001.txt\",'w')\n",
    "predFile.write(predSum)\n",
    "print(\"File Written to text.A.001.txt\")\n",
    "predFile.close()\n",
    "\n",
    "print(\"  Response Words: \",predSum)\n",
    "print(\"Time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
