{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating News headlines\n",
    "\n",
    "In this kernel, We will be using the dataset of News headlines fromt (https://www.kaggle.com/sunnysai12345/news-summary) to train a text generation language model which can be used to generate News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week-1\n",
    "\n",
    "### Data Preprocessing\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "import keras.utils as ku \n",
    "import keras.preprocessing.text as text\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset\n",
    "Read file which is in CSV (comma seperated values) format using pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset \n",
    "dataset = pd.read_csv(\"news_summary.csv\", delimiter=',', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#data sample\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4514 entries, 0 to 4513\n",
      "Data columns (total 6 columns):\n",
      "author       4514 non-null object\n",
      "date         4514 non-null object\n",
      "headlines    4514 non-null object\n",
      "read_more    4514 non-null object\n",
      "text         4514 non-null object\n",
      "ctext        4396 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 211.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get dataset Information\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning \n",
    "First, We will drop all the unnecessary columns and remove all puncuation and convert all headlines in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                                text  \n",
       "0  The Administration of Union Territory Daman an...  \n",
       "1  Malaika Arora slammed an Instagram user who tr...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Maharashtra will train their staff t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecessary cloumns from dataset\n",
    "dataset = dataset.drop([\"author\", \"date\", \"read_more\", \"ctext\"], 1)\n",
    "\n",
    "# After drop colums\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daman  diu revokes mandatory rakshabandhan in ...</td>\n",
       "      <td>the administration of union territory daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malaika slams user who trolled her for divorci...</td>\n",
       "      <td>malaika arora slammed an instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virgin now corrected to unmarried in igims form</td>\n",
       "      <td>the indira gandhi institute of medical science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaj aapne pakad liya let man dujana before bei...</td>\n",
       "      <td>lashkaretaibas kashmir commander abu dujana wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel staff to get training to spot signs of s...</td>\n",
       "      <td>hotels in maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  daman  diu revokes mandatory rakshabandhan in ...   \n",
       "1  malaika slams user who trolled her for divorci...   \n",
       "2    virgin now corrected to unmarried in igims form   \n",
       "3  aaj aapne pakad liya let man dujana before bei...   \n",
       "4  hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                                text  \n",
       "0  the administration of union territory daman an...  \n",
       "1  malaika arora slammed an instagram user who tr...  \n",
       "2  the indira gandhi institute of medical science...  \n",
       "3  lashkaretaibas kashmir commander abu dujana wh...  \n",
       "4  hotels in maharashtra will train their staff t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning, Remove puncuation and convert to lower case\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "dataset['headlines'] = [clean_text(txt) for txt in dataset['headlines']]\n",
    "dataset['text'] = [clean_text(txt) for txt in dataset['text']]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      "headlines    1000 non-null object\n",
      "text         1000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.iloc[:1000, :]\n",
    "dataset.info()\n",
    "\n",
    "dataset['text'][0] = dataset['text'][0] + \" tt \"\n",
    "dataset['headlines'][0] = dataset['headlines'][0] + \" tt \";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generating Sentence to Vector Using Tokenizer\n",
    "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Python’s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    return tokenizer, total_words\n",
    "\n",
    "# maximum length of input sentence\n",
    "max_Len_Input = max([len(text.text_to_word_sequence(line)) for line in dataset['text']])\n",
    "\n",
    "#maximum lenth of output sentence\n",
    "max_Len_out = max([len(text.text_to_word_sequence(line)) for line in dataset['headlines']])\n",
    "\n",
    "# tokenizer of input text\n",
    "tokenizer, total_words1 = get_sequence_of_tokens(dataset['text'])\n",
    "\n",
    "# dicitionary of input words\n",
    "inp_w2i_dict = tokenizer.word_index\n",
    "inp_i2w_dic = {}\n",
    "for word, index in inp_w2i_dict.items():\n",
    "        inp_i2w_dic[index] = word\n",
    "\n",
    "# input text vectorization\n",
    "inp_sequences = np.zeros(shape=(len(dataset['text']), max_Len_Input, total_words1))\n",
    "for i in range(len(dataset['text'])):\n",
    "    for j, word in enumerate(text.text_to_word_sequence(dataset['text'][i])):\n",
    "        inp_sequences[i,j,inp_w2i_dict[word]] = 1\n",
    "\n",
    "# tokenizer of output text\n",
    "tokenizer, total_words2 = get_sequence_of_tokens(dataset['headlines'])\n",
    "\n",
    "# dicitionary of output words \n",
    "out_w2i_dict = tokenizer.word_index\n",
    "out_i2w_dic = {}\n",
    "for word, index in out_w2i_dict.items():\n",
    "        out_i2w_dic[index] = word\n",
    "        \n",
    "# output sentence vectorization\n",
    "out_sequences = np.zeros(shape=(len(dataset['headlines']), max_Len_out, total_words2))\n",
    "target_data = np.zeros(shape=(len(dataset['headlines']), max_Len_out, total_words2))\n",
    "\n",
    "# output headline vectorization\n",
    "for i in range(len(dataset['headlines'])):\n",
    "    for j, word in enumerate(text.text_to_word_sequence(dataset['headlines'][i])):\n",
    "        out_sequences[i,j,out_w2i_dict[word]] = 1\n",
    "    if j > 0:\n",
    "        target_data[i, j-1, out_w2i_dict[word]] = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder_input = Input(shape=(None,total_words1))\n",
    "encoder_LSTM  = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]\n",
    "                      \n",
    "                      \n",
    "# Decoder model\n",
    "decoder_input = Input(shape=(None,total_words2))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(total_words2,activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# encoder_input_data & decoder_input_data into decoder_target_data`\n",
    "model = Model([encoder_input, decoder_input], decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 33s 42ms/step - loss: 0.6918 - val_loss: 0.6522\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.5656 - val_loss: 0.6686\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 18s 22ms/step - loss: 0.5157 - val_loss: 0.6898\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.5011 - val_loss: 0.7026\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4956 - val_loss: 0.7237\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 15s 18ms/step - loss: 0.4914 - val_loss: 0.7006\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.4892 - val_loss: 0.7570\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.4914 - val_loss: 0.7448\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.4810 - val_loss: 0.7483\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4758 - val_loss: 0.7496\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 15s 18ms/step - loss: 0.4741 - val_loss: 0.7426\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4687 - val_loss: 0.7644\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4638 - val_loss: 0.8071\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4600 - val_loss: 0.7722\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4571 - val_loss: 0.7353\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4533 - val_loss: 0.7943\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4536 - val_loss: 0.8211\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4460 - val_loss: 0.7644\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.4419 - val_loss: 0.8158\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4370 - val_loss: 0.7987\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 15s 18ms/step - loss: 0.4358 - val_loss: 0.7395\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4297 - val_loss: 0.7977\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4245 - val_loss: 0.8071\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4229 - val_loss: 0.7910\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4150 - val_loss: 0.8523\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 15s 18ms/step - loss: 0.4061 - val_loss: 0.8184\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4027 - val_loss: 0.8185\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3944 - val_loss: 0.8231\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.3868 - val_loss: 0.8567\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.3763 - val_loss: 0.8792\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3639 - val_loss: 0.8190\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 14s 18ms/step - loss: 0.3536 - val_loss: 0.8617\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3354 - val_loss: 0.8422\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3199 - val_loss: 0.8606\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3069 - val_loss: 0.8650\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.2822 - val_loss: 0.7702\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2672 - val_loss: 0.8494\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.2484 - val_loss: 0.8854\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2299 - val_loss: 0.8717\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2048 - val_loss: 0.8623\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.1908 - val_loss: 0.8578\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.1697 - val_loss: 0.8777\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.1515 - val_loss: 0.8701\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.1299 - val_loss: 0.8782\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.1129 - val_loss: 0.8746\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.0943 - val_loss: 0.8820\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.0785 - val_loss: 0.8902\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.0633 - val_loss: 0.8934\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.0509 - val_loss: 0.8989\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.0402 - val_loss: 0.9202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24d19710588>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[inp_sequences,out_sequences], \n",
    "          y=target_data,\n",
    "          batch_size=64,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, total_words2))\n",
    "    target_seq[0, 0, out_w2i_dict['tt']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = out_i2w_dic[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_Len_out)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, total_words2))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = decode_seq(inp_sequences[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a mumbai court has convicted 15 somali pirates to 7 years of imprisonment in a 2011 case the pirates were found guilty of attempt to murder and kidnapping for taking 22 people hostage on board a commercial ship from thailand this is one of the four cases registered against 120 somali pirates for holding 91 people from different countries hostage'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
